\documentclass{article}

\begin{document}
\SweaveOpts{concordance=TRUE}
%\SweaveOpts{concordance=TRUE}

\section{The AR2 Model}

Consider a mean zero AR(2) model conditioned on $y_1$ and $y_2$.

$$y_t = \phi_1 * y_{t-1} + \phi_2 * y_{t-2} + \epsilon_t \mbox{ for } t = 3,\dots,T$$,

where $\epsilon$ are iid $\mathcal{N}(0, \sigma^2)$.

Using the independent priors: 

\begin{eqnarray*}
p(\phi_1) & \sim & \mathcal{N}(\bar{phi_1}, \tau_{1}^2) \\
p(\phi_2) & \sim & \mathcal{N}(\bar{phi_2}, \tau_{2}^2) \\
p(\sigma^2) & \sim & \mathcal{IG} (\mbox{shape} = \alpha, \mbox{ rate} = \beta),
\end{eqnarray*}
we have a model with zero stationary restrictions that can be easily sampled through Gibbs MCMC or approximated by a range of variational techniques.

There are five approaches considered:

\begin{itemize}
\item Gibbs MCMC
\item Mean field Variational Bayes
\item Stochastic Variational Bayes
\item Stochastic Variational Bayes with Control Variates
\item Re-parameterised Stochastic Variational Bayes
\end{itemize}

Simulation proceeds with $T = 100$, $\phi_1 = 0.7$, $\phi_2 = 0.15$ and $\sigma^2 = 2$. Each approach estimates the same approximating distribution, $q(\theta) = BVN((\phi_1, \phi_2)' | \mu, \Sigma) IG(\sigma^2 | \alpha, \beta)$ with estimated parameters:

\begin{itemize}
\item $\mu_1$: The mean of $\phi_1$
\item $\mu_2$: The mean of $\phi_2$
\item $L_{11}, L_{21}, L_{22}$, the three components of the lower triangular decomposition of $\Sigma$
\item $\alpha, \beta$, the shape and rate parameters of $\sigma^2$.
\end{itemize}

The diagonal of $L$ is not forced to be positive so the decomposition is not invariant to sign changes.

Collectively, we have $\lambda = (\mu_1, \mu_2, L_{11}, L_{21}, L_{22}, \alpha, \beta)'$.

\begin{figure}
<<MCMC>>=
setwd("/home/nltom2/Desktop/Confirmation")
library(ggplot2)
library(GGally)
thin <- read.csv("AR2thin.csv")[,-1]
ggpairs(thin)
@
\caption{Result of MCMC with 120,000 draws. The first 20\% were discarded then the remaining draws were thinned by a factor of 20, which was required to remove any dependence. The remaining effective sample size for each parameter is 5000.
\end{figure}


<<summary>>=
library(knitr)
library(mvtnorm)
library(reshape)
summary = matrix( c(0.78, 0.16, 0.10, -0.09, 0.05, 48.9, 77.3, -76.7, 2.6,
                    0.83, 0.10, 0.04, 0, 0.05, 50, 79.7, -77.7, "1.3 x 10^{-6}", 
                    0.63, 0.27, 0.09, -0.12, 0.05, 45.6, 81.4, -78.3, 51.1, 
                    0.78, 0.15, 0.1, -0.09, 0.05, 49.3, 78, -76.7, 4.0,
                    0.77, 0.15, 0.09, -0.09, 0.05, 48.5, 77.6, -76.9, 1.3) , byrow = TRUE, ncol = 9)
                    
colnames(summary) = c(paste0(expression(mu), 1), paste0(expression(mu), 2), "L11", "L21", "L22", expression(alpha), expression(beta), "L(theta)", "time (seconds)*")
rownames(summary) = c("Method of Moments", "Mean Field", "SVB", "SVB CV", "SVB RP")
kable(summary)
@
The original stochastic variational bayes algorithm converged to a different local maximum, possibly due to the high variance in its gradient estimates.

Note that all of the variational algorithms are currently unparallelised, and the stochastic versions in particular have a huge potential for faster performance.

\end{document}