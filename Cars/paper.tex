  \documentclass[12pt,a4paper]{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\usepackage{alltt}%
\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)

%2multibyte Version: 5.50.0.2960 CodePage: 1252
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[centertags]{amsmath}
\usepackage{graphicx}%
\usepackage{natbib}
\usepackage{color}
\usepackage[dvipsnames,svgnames*]{xcolor}
\usepackage{array}
\usepackage[hidelinks]{hyperref}
\usepackage[font=small,skip=5pt]{caption}
\usepackage[aboveskip=2pt]{subcaption}
\usepackage{amsmath}
\usepackage{amsthm}
%\usepackage{tikz}
%\usetikzlibrary{bayesnet}
\usepackage{url}
\usepackage{ulem}
\usepackage{afterpage}
\setcounter{MaxMatrixCols}{30}

\setlength{\topmargin}{0in}
\setlength{\oddsidemargin}{0.1in}
\setlength{\evensidemargin}{0.1in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8.25in}
\setlength\parindent{0pt}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}

\title{Online Updating of Variational Bayes for Heterogenous Forecasts of Vehicle Trajectory}
\begin{document}
\maketitle
\section{Introduction}

\begin{itemize}
\item Self driving Cars are becoming more common, eg Waymo, Tesla, GM.
\item The naviation systems used to drive involve predicting the movement of the surrounding traffic to find the safest path for the Self Driving Car.
\item While Self Driving Cars reduce collisions due to human error, safety can be further improved by better forecasts of the trajectory of nearby human controlled vehicles.
\item These models use deep neural networks (DNN) to extract the properties of surrounding vehicles from sensors (eg. Cameras and LiDAR), such as their position, velocity, and steering angle (Woo et al. 2016a; Tian, Pei, Jana, and Ray 2017).
\item The Self Driving Car forecasts point estimates of the future trajectory of these surrounding vehicles based on the DNN results.
\item These can assume the surrounding vehicles will maintain their current angle and velocity (Gindele, Brechtel, Dillmann 2010; Waymo Safety Report 2017; Bautista 2017), but interest has developed in more advanced predictive models such as Neural Networks, Hidden Markov Models, and Support Vector Machines (Woo et al. 201b6; Ding et al. 2012; Woo et al. 2017; Geng et al. 2017).
\item This paper takes a statistical approach, fitting time series models to two extracted variables, velocity and steering angle, of vehicles in the Next Generation Simulation (NGSIM) US Highway 101 Dataset.
\item It then uses Bayesian methods to forecast the distribution of the future velocity and steering angle, which can be transformed into a trajectory forecast.
\item A homogenous time series model shows a strong ability to predict the MAP trajectory
\itme An important part of this modelling is to capture the uncertainty of car trajectories, as the self-driving car must react to the possibility of collision even when that possibility is relatively small.
\item There are many sources of heterogeneity in drivers, from their individual driving styles to the conditions of traffic directly around them.
\item Including heterogeneity in the model will allow the model to better adapt to drivers with more extreme behaviours, for example 'lead-footed' drivers can be assigned a larger variance on their acceleration while forecasts for more consistent drivers will benefit from a low variance.
\item A hierarchical model is employed to allow heterogeneity between different drivers, where each vehicle is given its own parameter vector with a shared prior distribution.
\item This model implies that any new vehicle observed by the self driving car has its own parameter vector, related to previously observed vehicles through the shared prior.
\item The self driving car must be able to estimate the parameter vector of newly observed cars, and update this parameter vector as it observes additional data.
\item Forecasts where the parameter vector is conditioned on this additional data should outperform forecasts conditioned on less data.
\item A three step strategy is employed to provide density forecasts for the future trajectory of newly observed vehicles:
\begin{enumerate}
\item Fit a hierarchical model with a large amount of vehicles to obtain the posterior distribution of each vehicles individual parameter vector and the shared distribution. This can occur before the self driving car is on the road.
\item When a new vehicle is observed use the information in this shared distribution to model the associated new parameter vector. 
\item Update the parameter vector and all forecasts at short intervals.
\end{enumerate}
\item This paper uses standard Variational Bayes for the second step, and introduces an updating method for the third step; the posterior distribution of the new vehicles parameter vector is approximated, then this approximation is updated.
\item This only uses the data observed in-between updates, allowing the previously observed history of that vehicle to be discarded.
\item This is important for two reasons:
\begin{enumerate}
\item The model can be updated faster as less data must be processed, so forecasts can be made using as much recent data as possible.
\item Self driving cars take in up to 1GB of data a second (several websites report this as quotes from different people eg. Tech CEO, but I don't know if that's good enough for an academic citaiton), so discarding old data is useful.
\end{enumerate}
\item Show that a homogenous time series model has strong predictive performance compared to constant models / neural networks while also providing density forecasts.
\item Build a heterogenous version of the homogenous model, show that the density forecasts improve. 
\item Show that the Updating VB required to implement the heterogenous model in real time is comparable to MCMC and standard VB.
\end{itemize}

\section{Data processing}

\begin{itemize}
\item Data is provided by the Next Generation Simulation (NGSIM) project, which observes cars for a 2000 feet section of the US 101 Highway.
\item Observations of x/y coordinates are provided every 100 ms (10 hz), with around 500-1000 observations per car.
\item Around 2000 of these cars changed lanes or stopped moving somewhere in the sample and are discarded.
\item There was some curvature in the road which means there are systematic movements in the car paths.
\item Modern cars are able to track lane marking and can calculate distance to other cars in relative co-ordinates (Thuy and Leon 2010), instead of relative to the Earth (ie. ignores curves)
\item Prior research fits a polynomial curve to detected lane markings to build a model of the road lane edges (Woo et. al. 2016) 
\item Instead, I fit splines to the positions of each car to build an estimate of the road midpoint. Relative co-ordinates are found separately for each lane using the movements of 100 cars / lane by the following procedure:
\begin{enumerate}
\item At each point in time $t$ calculate the total distance each car has travelled along the road using $d_{i, t} = \sum_{s=1}^t v_{i, s}$, where $v_{i, s}$ is the velocity of car $i$ at time $s$. 
\item Estimate independent smoothing splines of the form $x_{i, t} = f(d_{i, t})$ and $y_{i, t} = g(d_{i, t})$, where $x_{i, t}$ and $y_{i, t}$ are the car coordinates relative to the start of the road.
\item Use models to get $\hat{x}_{i, t}$ and $\hat{y}_{i, t}$ as estimates of the midpoint of the lane after travelling a distance $d_{i, t}$.
\item Define new coordinates 
\begin{align}
x^*_{i, t} &= \mbox{sign}\left(\tan^{-1}\left(\frac{\hat{y}_{i, t} - y_{i, t}}{\hat{x}_{i, t} - x_{i, t}} \right) - \tan^{-1}\left(\frac{g'(d_{i, t}) }{f'(d_{i, t})}\right)\right)\sqrt{(x_{i, t}-\hat{x}_{i, t})^2 + (y_{i, t} - \hat{y}_{i, t})^2)} \label{xRel} \\
y^*_{i, t} &= d_{i, t} \label{yRel}
\end{align}
\end{enumerate}
\item (\ref{xRel}) \textit{really} needs a diagram.
\item These cars are not used for any further modelling, leaving 2873 cars travelling on a straightened out road with no noticable systematic curvature.
\end{itemize}

\section{Hierarchical Motion Model}

The position of any vehicle at time $t$ can be fully determined by its initial position, $\{x^*_{i, 0}, y^*_{i, 0}\}$ and the history of the driver's inputs: the vehicle's velocity $v_{i, t}$ and steering angle, $\delta_{i, t}$ by
\begin{align}
x^*_{t} &= x^*_{t-1} + v_{t} \cos(\delta_{t}) \label{xEq} \\
y^*_{t} &= y^*_{t-1} + v_{t} \sin(\delta_{t}) \label{yEq},
\end{align}
where $\delta_{t} = \pi/2$ denotes that the car has no lateral relative movement and $x^*_{t} = x^*_{t-1}$.
From this relationship, the driver's key inputs to motion can be extracted by
\begin{align}
\delta_{t} &= \tan^{-1}\left(\frac{(y^*_{t} - y^*_{t-1})}{(x^*_{t} - x^*_{t-1})} \right) \label{dEq} \\
v_{t} &= \sqrt{(x^*_{t} - x^*_{t-1})^2 + (y^*_{t} - y^*_{t-1})^2} \label{vEq} \\
a_{t} &= v_{t} - v_{t-1}. \label{aEq}
\end{align}
where $a_{t}$ denotes the acceleration at time $t$.

Auto-regressive processes of orders $p$ and $q$ for $a_t$ and  $\delta_t$ are used to model the level of persistance and variation of driver actions, with
\begin{align}
a_{t} &= \sum_{j = 1}^p \phi_{j} a_{t-j} + \sigma_{\epsilon} \epsilon_{t} \label{aAR} \\
\delta_{t} &= \pi/2 + \sum_{j = 1}^q \gamma_{j} (\delta_{t-j} - \pi/2) + \sigma_{\eta} \eta_{t} \label{dAR}
\end{align}
where $\epsilon_{t}$ and $\eta_t$ are independently and identically distributed according to a standard normal distribution. The unconditional means of $a_t$ and $\delta_t$ are set to zero and $\pi/2$ respectively, corresponding to forward motion at constant velocity. The parameter vector to be estimated is collected as 
\begin{equation*}
\label{thetaVec}
\theta = \{\phi_{1}, \dots, \phi_{p}, \gamma_{1}, \dots, \gamma_{q}, \log(\sigma^{2}_{\epsilon}), \log(\sigma^{2}_{\eta})\} \in \mathbb{R}^{p + q + 2}.
\end{equation*}

Multiple groups of differing driver behaviour can be allowed while still retaining heterogeneity within each group by jointly estimating each vehicle driver's individual parameter vector, $\theta_i$, as part of a heirarchical model. In this case each $\theta_i$ is treated as an independent draw from a $K$ component mixture of multivariate normal distributions, augmenting each $\theta_i$ with a mixture component $k_i = 1, \dots, K$ such that 
\begin{equation}
\theta_i | k_i = j \sim N(\mu_j, \Sigma_j).
\end{equation}
For each $j = 1, \dots, K$, multivariate normal priors are chosen for $\mu_j$ with fixed mean $\bar{\mu}_j$ and variance matrix $\Omega_j$, Inverse Wishart priors are chosen for $\Sigma_j$ with fixed degrees of freedom $\tau_j$ and scale matrix $\Psi_j$. $k_{i}$ has a multinomial distribution with mixture probabilities $\boldsymbol{\pi} = \{\pi_{1}, \dots, \pi_{K}\}$, where $\boldsymbol{\pi}$ has a Dirichlet prior with fixed hyperparameters $\alpha = \alpha_j, j = 1 , \dots, K$. Defining $\beta = \{\mu_j, \Sigma_j, \pi_j, j = 1, \dots, K\}$, $z_{i, 1:T} = \{x^*_{i, s}, y^*_{i, s} | s = 1, \dots, T\}$ and $z_{1:N} = \{z_{i, 1:T} | i = 1, \dots, N\}$ the posterior distribution $p(\theta_1, \dots, \theta_N, \beta | z_{1:N})$ can be estimated with MCMC methods.

\begin{figure}[ht]
\centering
\includegraphics[width = 0.95\textwidth]{HierSingleKDE.png}
\caption{Top: A Kernel Density Estimate applied to the individual posterior means for each $\theta_i | z_i, i = 1, \dots, 2000$ where $p = q = 2$, modelled independently without any hierarchical structure. Bottom: $p(\theta_i | \beta, z_{1:2000})$ for a hierarchical mixture model with $K = 6$ using the same vehicles, with each component of the mixture denoted by the coloured densities. The hierarchical model has captured the bimodality and skewness present in the independent posterior means.}
\label{fig:HierSingleKDE}
\end{figure}

\section{Variational Bayes}

\subsection{Background}

Variational Bayes (VB, Jordan et al. 1999) approximates a distribution $p(\theta | z)$ with a family $q(\theta)$, choosing the member of that family that minimises $KL(q(\theta) || p(\theta | z)$, where
\begin{equation}
\label{KL-div}
KL(q(\theta) || p(\theta | z) = \int_{\theta} q(\theta) \left(\log(q(\theta)) - \log(p(\theta | z))\right) d\theta.
\end{equation}
As $p(\theta | z)$ is typically intractable, VB maximises $\mathcal{L}(q)$ where
\begin{equation}
\label{ELBO}
\mathcal{L}(q, \lambda) = \int_{\theta} q(\theta) \left(\log(p(\theta, z)) - \log(q(\theta))\right) d\theta
\end{equation}
which is equivalent to minimising (\ref{KL-div}), where $\lambda$ is the parameter vector of $q(\theta)$.
This is done with stochastic gradient ascent, taking Monte Carlo estimates of $\delta\mathcal{L}(q, \lambda) / \delta \lambda$ (Hoffman et al. 2013)
and repeating updates of the form
\begin{equation}
\label{gradientAscent}
\lambda^{(m+1)} = \lambda^{(m)} + \rho^{(m)} \frac{\delta\mathcal{L}(q, \lambda)}{\delta \lambda} 
\end{equation}
which is guaranteed to converge to a local maximum (Robbins and Munro, 1951) if $\rho^{(m)}, m = 1, \dots, \infty$ satisfies
\begin{align}
&\sum_{m=1}^{\infty} \rho^{(m)} =  \infty \\
&\sum_{m=1}^{\infty} (\rho^{(m)})^2 <  \infty.
\end{align}
In this paper the sequence $\rho^{(m)}$ is provided by the Adam algorithm of Kingma and Ba (2015).

There are two Monte Carlo estimators of $\delta\mathcal{L}(q, \lambda) / \delta \lambda$, the score estimator of Ranganath, Gerrish and Blei (2014), 
\begin{equation}
\label{scoreDeriv}
\frac{\delta\mathcal{L}(q, \lambda)}{\delta \lambda} \approx \sum_{i = 1}^L \frac{\delta \log(q(\theta_i))}{\delta \lambda} \left(\log(p(\theta_i, z)) - \log(q(\theta_i)) \right), \mbox{ where } \theta_i \sim q(\theta),
\end{equation}
and the reparameterised estimator of Kingma and Welling (2014),
\begin{equation}
\label{rpDeriv}
\frac{\delta\mathcal{L}(q, \lambda)}{\delta \lambda} \approx \sum_{i = 1}^M \frac{\delta f(\lambda, \epsilon_i)}{\delta \lambda} \frac{\delta \log(p(\theta, z))}{\delta \theta} \bigg\rvert_{\theta = f(\lambda, \epsilon_i)} + \frac{\delta J(\lambda, \epsilon_i)}{\delta \lambda}, \mbox{ where } \epsilon_i \sim r(\epsilon)
\end{equation}
where $r(\epsilon)$ is some distribution without any free parameters (eg. Uniform, Standard Normal), $f(\lambda, \epsilon)$ is some differentiable function that transforms $\epsilon$ to $\theta$, and $J(\lambda, \epsilon)$ is the Jacobian Matrix of this function. (\ref{rpDeriv}) typically has significantly lower variance than (\ref{scoreDeriv}), allowing $M$ to be set lower than $L$ which improves the speed of the algorithm; however the requirement for $f$ to exist restricts the class of approximating distributions $q(\theta)$ that can be used.
Notably, a differentiable $f(\lambda, \epsilon)$ does not exist when $q(\theta)$ is a mixture model and the score estimator (\ref{scoreDeriv}) must be used. To offset the increased variance and allow a reduction in $L$, following Gunawan, Tran, and Kohn (2017), Randomised Quasi Monte Carlo (RQMC) is used, where numbers in the unit hypercube are generated according to a Sobol Sequence (Sobol 1967), then these are randomised using the scrambled net method (Matousek 1998), and transformed into a draw of $\theta$ using the inverse-CDF of $q(\theta)$. Using RQMC allows (\ref{scoreDeriv}) to converge to the true derivative at a rate $O(L)$ compared to $O(\sqrt{L})$ in standard Monte Carlo. 

\subsection{Updating VB Process}

\begin{itemize}
\item While on the road a self driving car may wish to make posterior inferences on the parameter set of cars around them. They may be interested in the degree of erraticness / variance of their actions.
\item The posterior for car $N+1$ is given by 
\begin{equation}
\label{originalPost}
p(\theta_{N+1} z_{N+1} | z_{1:N}) \propto p(z_{N+1} | \theta_{N+1}) \int_{\beta} p(\theta_{N+1} | \beta) p (\beta | z_{1:N}) d\beta
\end{equation}
\item As N is large $p (\beta | z_{1:N})$ is fairly precise, so we can replace $\int_{\beta} p(\theta_{N+1} | \beta) p (\beta | z_{1:N}) d\beta$ with $ p(\theta_{N+1} | \hat{\beta})$ where $\hat{\beta}$ is the posterior mean of $p(\beta | z_{1:N})$. This approximation gives us a closed form for $ p(\theta_{N+1} | \hat{\beta})$.
\item Subsequently fitting $p(\theta_{N+1} | z_{1:N}, z_{N+1, 1:S})  \propto p(z_{N+1} | \theta_{N+1}) p(\theta_{N+1} | \hat{\beta})$ is fairly easy with MCMC / VB etc.
\item When $S$ is small, the information in the prior will dominate the individual information.
\item However, for some $T > S$ the information in the car's individual movements should become more relevant and forecasts could be improved by updating the distribution to $p(\theta_{N+1} | z_{1:N}, z_{N+1, 1:T})$.
\item This updated posterior is given by
\begin{equation}
\label{updatePost}
p(\theta_{N+1} | z_{N+1, 1:T}, z_{1:N}) \propto p(z_{N+1, S+1:T} | \theta_{N+1})p(\theta_{N+1} | z_{1:N}, z_{N+1, 1:S})
\end{equation}
\item Both $\mathcal{L}(q)$ derivative estimators, (\ref{scoreDeriv}) and (\ref{rpDeriv}), need to evaluate \newline $\log(p(\theta_{N+1}, z_{N+1, S+1:T} | z_{1:N}, z_{N+1, 1:S}))$, which takes the form
\begin{align}
\log(p(\theta_{N+1}, z_{N+1, S+1:T} | z_{1:N}, z_{N+1, 1:S})) &= \log(p(z_{N+1, S+1:T} | \theta_{N+1})) \nonumber \\
&+ \log(p(\theta_{N+1} | z_{1:N}, z_{N+1, 1:S})) \label{updateEq}
\end{align}
\item To perform this update only using the data from $z_{N+1, S+1:T}$ we would need a closed form form expression for $p(\theta_{N+1} | z_{1:N}, z_{N+1, 1:S})$ to replace the role of the prior. Without this closed form we would need to re-fit the VB model using the original prior $p(\theta_{N+1} | \hat{\beta})$ and the whole observed sequence $z_{N+1, 1:T}$.
\item Due to the demands of driving, the car wants the update carried out as soon as possible, and ideally should be able to discard the previously used information $z_{N+1, 1:S}$. 
\item Defining $q(\theta_{N+1} | z_{N+1, 1:S})$ as the Variational Bayes approximation of $p(\theta_{N+1} | z_{1:N}, z_{N+1, 1:S})$, a new method is proposed to update $q(\theta_{N+1} | z_{N+1, 1:S})$ to $q(\theta_{N+1} | z_{N+1, 1:T})$ after observing $z_{N_1, S+1:T}$.
\item By replacing $p(\theta_{N+1} | z_{1:N}, z_{N+1, 1:S})$ in (\ref{updateEq}) with the previously fit $q(\theta_{N+1} | z_{N+1, 1:S})$ we get an approximation to (\ref{updateEq}):
\begin{equation}
\label{updateEqApprox}
\log(p(\theta_{N+1}, z_{N+1, 1:T} | z_{1:N})) \approx \log(p(z_{N+1, S+1:T} | \theta_{N+1})) + \log(q(\theta_{N+1} | z_{N+1, 1:S}))
\end{equation}
\item Substituting (\ref{updateEqApprox}) into (\ref{scoreDeriv}) gives the score estimator for updating Variational Bayes, 
\begin{align}
\label{scoreUpdate}
\frac{\delta\mathcal{L}(q, \lambda)}{\delta \lambda} \approx \sum_{i = 1}^L \frac{\delta \log(q(\theta_{N+1, i} | z_{N+1, 1:T}))}{\delta \lambda} &\left(\log(q(\theta_{N+1, i} | z_{N+1, 1:S}) - \log(q(\theta_{N+1} | z_{N+1, 1:T})) \right), \nonumber \\
& \mbox{ where } \theta_{N+1, i} \sim q(\theta_{N+1} | z_{N+1, 1:T}).
\end{align}
\item Similarly, substituting (\ref{updateEqApprox}) into (\ref{rpDeriv}) gives the reparameterised estimator for updating Variational Bayes,
\begin{equation}
\label{rpUpdate}
\frac{\delta\mathcal{L}(q, \lambda_T)}{\delta \lambda_T} \approx \sum_{i = 1}^M \frac{\delta f(\lambda_T, \epsilon_i)}{\delta \lambda_T} \frac{\delta \log(q(\theta_{N+1} | z_{N+1, 1:S}))}{\delta \theta_{N+1}} \bigg\rvert_{\theta_{N+1} = f(\lambda_T, \epsilon_i)} + \frac{\delta J(\lambda_T, \epsilon_i)}{\delta \lambda_T}, \mbox{ where } \epsilon_i \sim r(\epsilon)
\end{equation}
where $\lambda_T$ denotes the parameter vector of $q(\theta_{N+1} | z_{N+1, 1:T})$.
\item This process can be repeated as neccessary, and is $O(1)$ instead of $O(T)$ assuming $S$ grows at the same rate as $T$.
\end{itemize}

\section{Forecast Evaluation}

Summary:
\begin{itemize}
\item The homogenous time series model has a significantly lower MAP error than other models
\item The heterogenous time series models have a similar MAP error to the homogenous model - the maximum of the forecast distribution is in a similar place.
\item The logscores for the heterogenous models are generally greater than the homogenous model indicating there is less uncertainty in the forecast distribution
 \item Updating VB has a similar logscore to MCMC across all priors
 \item Priors don't really matter too much
 \item The high precision in the homogenous posterior does make it better than the heterogenous models in small samples.
 \item Performance between homogenous and heterogenous becomes a tradeoff between high precision / biased mean vs. low precision, more accurate mean in the posterior.
 \item The heterogenous is sufficiently precise to outperform the homogenous model in terms of logscores after $T = 100$ (ten seconds).
 \item Most of the differences are due to the posterior of the variance parameters
 \item The homogenous model has a large variance for both variables, greater than $80\%$ and $92\%$ of heterogenous variances for acceleration and angle respectively.
 \item This is likely caused by a few drivers with high variance skewing the results in the homogenous model.
 \item The heterogenous model allows consistent drivers to have a low variance, and it performs very well in this case.
 \item Heterogenous models being able to assign a high variance allows more erratic / risky drivers to be identified.
 \end{itemize}



\begin{itemize}
\item 3873 vehicles are available in the sample
\item 500 are used to estimate splines and straighten the road to relative coordinates as described in Section 2
\item 2000 are used to train any models used
\item 1373 are available for testing
\item Each second (10 observations) models are fit to the sample of $z_{N+1, 1:T}$ and forecasts of $z_{N+1, T+1:T+30}$ are made, corresponding to a three second forecast horizon
\item Forecasts are compared in terms of predictive log-score (where a predictive density is available) and Euclidean error.
\end{itemize}

\subsection{Heterogenous Time Series Models}

There are three versions of the heterogenous time series model described in Section 3, differentiated by the form of the prior, $p(\theta_{N+1} | z_{1:N})$.
\begin{enumerate}
\item Non-informative prior for the new car, where $p(\theta | z_{1:N})$ is multivariate normal with mean vector $(-5, -5, 0, 0, 0, 0)$ and diagonal variance matrix $10 \mathbb{I}$.
\item Informative prior where $p(\theta | z_{1:N})$ is multivariate normal with a mean vector and variance matrix estimated by the posterior mean of $\mu$ and $\Sigma$ in a hierarchical model with $K = 1$ fit by MCMC to the 2000 training vehicles. 
\item Informative prior where $p(\theta | z_{1:N})$ is a six component mixture of multivariate normals with mean vectors, variance matrices and mixture weights estimated by the posterior means of each $\mu_j$ and $\Sigma_j$ and $\pi_j$ in a hierarchical model with $K = 6$ fit by MCMC to the 2000 training vehicles. 
\end{enumerate}
Each model is fit by each of Metropolis-Hasting MCMC, standard Variational Bayes where the entire observed history of $z_{N+1, 1:T}$ is used for each model fit, and the updating Variational Bayes process described by Section 4.2 where only the most recent observed data $z_{N+1, S+1:T}$ is used in each iteration. In each Variational Bayes algorithm the approximating distribution has the same functional form as the prior used, a multivariate normal distribution for the non-informative prior and $K = 1$ hierarchy, and a six component mixture of multivariate normals for the $K = 6$ hierarchy. Details of each case are provided in Table \ref{tableAlg}.
\begin{center}
\begin{table}[h]
\resizebox{\textwidth}{!}{%
\begin{tabular}{| l | c | c | c |}
\hline
& Non-Informative Prior & Simple Hierarchy & Finite Mixture Hierarchy \\
\hline
MCMC & \multicolumn{3}{c|}{Random Walk Metropolis Hastings jointly drawing the entire $\theta$ vector using Garthwaite, Fan and Sisson (2015)} \\
& \multicolumn{3}{c|}{to control the variance matrix of the Multivariate Gaussian proposal distribution} \\
\hline
VB  & \multicolumn{2}{c|}{Variational Bayes using (\ref{rpDeriv}) to update parameters of a} & Variational Bayes using (\ref{scoreDeriv}) to update parameters of a six \\
Standard & \multicolumn{2}{c|}{Multivariate Gaussian approximation with non-zero covariance} & component mixture of diagonal variance Multivariate Gaussians\\
\hline
VB & \multicolumn{2}{c|}{Variational Bayes using (\ref{rpUpdate}) to update parameters of a} &  Variational Bayes using (\ref{scoreUpdate}) to update parameters of a six \\
Updating & \multicolumn{2}{c|}{Multivariate Gaussian approximation with non-zero covariance} & component mixture of diagonal variance Multivariate Gaussians\\ 
\hline
\end{tabular}}
\label{tableAlg}
\caption{Details of the algorithms used to produce posterior distributions for each method and prior combination}
\end{table}
\end{center}

\subsection{Competing Models}

\subsubsection{Constant Angle and Acceleration Models}

The framework provided by (\ref{xEq}) and (\ref{yEq}) allows point estimates of $\{x^*_{t}, y^*_{t} | t = T + 1, \dots, T+30\}$ to be easily obtained given $\{x^*_T, y^*_T, v_T\}$ and point estimates of the future values of $\{a_{t}, \delta_{t} | t = T + 1, \dots, T+30\}$. Nine naive forecast models are provided where these future values are estimated by a constant $\{\hat{a}, \hat{\delta}\}$. The classification of each naive model is provided in Table \ref{tableNaive} as a combination of one estimator for $\hat{a}$ and one estimator for $\hat{\delta}$. Acceleration estimators are: $\hat{a} = a_T, \hat{a} = 0.1 \sum_{s = T-9}^T a_s$, and $\hat{a} = 0$. Similarly steering angle estimators are: $\hat{\delta} = \delta_T, \hat{\delta} = 0.1 \sum_{s = T - 9}^T \delta_s$, and $\hat{\delta} = \pi/2$.
\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
& $\hat{\delta}_t = \delta_T$ & $\hat{\delta}_t = 1/10 \sum_{s=T-9}^T \delta_s$ & $\hat{\delta}_t = \pi/2$ \\
\hline
$\hat{a}_t = a_T$ & Naive 1 & Naive 2 & Naive 3 \\
$\hat{a}_t = 1/10\sum_{s=T-9}^T a_s$ & Naive 4 & Naive 5 & Naive 6\\
$\hat{a}_t = 0$ & Naive 7 & Naive 8 & Naive 9 \\
\hline
\end{tabular}
\end{center}
\label{tableNaive}
\caption{Classification of naive forecast models by future velocity and steering angle estimators}
\end{table}

\subsubsection{Homogenous Time Series Models}

The homogenous time series model has the same bivariate AR(2) structure given by (\ref{aAr}) and (\ref{dAr}) as used by the heterogenous time series models with the additional restriction that each $\theta_i = \theta$, so each vehicle shares the same parameter vector. This model is estimated by MCMC to the 2000 training cars.

\subsubsection{Recurrent Neural Networks}

A Recurrent Neural Network is constructed with inputs given by the ten most recent lags of vehicle position, angle, and velocity, and an output given by the next thirty values of the vehicle position. The network is trained with the Adam optimiser and a mean Euclidean error loss function. 
\subsection{Results}

\begin{figure}[ht]
\centering
\includegraphics[width = 0.95\textwidth]{CarsBoxplots.png}
\caption{Predictive logscores for $p(z_{i, T+h} | z_{i, 1:T})$ for $h = 1, \dots, 30$ and $T = \{10, 20, \dots, 300\}$ for each of the three levels of prior information fit by MCMC, standard VB and updating VB. The median logscore across all time periods, forecast horizons, and priors for the updating VB algorithm is 3.28, compared to 3.43 for standard VB and 3.46 for MCMC.}
\label{fig:MainResults}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width = 0.95\textwidth]{diffInLS.png}
\caption{A truncated view of the difference in Variational Bayes predictive logscores (updating minus standard) for each prior and a range of different T values. $99\%$ of Finite Mixture differences, $93\%$ of Non-Informative differences and $92\%$ of Single Hierarchy differences are inside the (-5, 5) region. The algorithms are identical when $T = 10$ so this is excluded. Updating VB logscores are only slightly lower than the standard VB logscores, signifying only a small loss of accuracy associated with the extra approximation.}
\label{fig:diffInLS}
\end{figure}

\begin{center}
\begin{table}[h]
\begin{tabular}{| l | c | c | c |}
\hline
& Non-Informative Prior & Simple Hierarchy & Finite Mixture Hierarchy \\
\hline
MCMC & 0.\textbf{846} & 0.899 & \textbf{0.931} \\
VB Standard & 0.782 & \textbf{0.913} & 0.901 \\
VB Updating & 0.558 & 0.701 & 0.873 \\
\hline
\end{tabular}
\label{tableMedian}
\caption{Median predictive logscore for $p(z_{i, T+h} | z_{i, 1:T})$ for $h = 1, \dots, 30$ and $T = \{10, 20, \dots, 300\}$ for each prior and algorithm combination}
\end{table}
\end{center}

\begin{center}
\begin{table}[h]
\begin{tabular}{| l | c | c | c |}
\hline
& Non-Informative Prior & Simple Hierarchy & Finite Mixture Hierarchy \\
\hline
MCMC & 0.209 & 0.199 & 0.197 \\
VB Standard & \textbf{0.203} & \textbf{0.196} & 0.197 \\
VB Updating & 0.211 & 0.207 & \textbf{0.196} \\
\hline
\end{tabular}
\label{tableError}
\caption{Mean predictive Euclidean error in metres using the maximum of $p(z_{i, T+h} | z_{i, 1:T})$ as a point-estimate with $h = 1, \dots, 30$ and $T = \{10, 20, \dots, 300\}$ for each algorithm and prior combination. Updating the VB model is competitive with a standard VB fit and exact sampling by MCMC.}
\end{table}
\end{center}

\begin{figure}[t]
\centering
\includegraphics[width = 0.95\textwidth]{PredictiveError.png}
\caption{Mean predictive Euclidean error in metres for $p(z_{i, T+h} | z_{i, 1:T})$ for $h = \{10, 20, 30\}$ and $T = \{10, 20, \dots, 300\}$ for each model. Each AR(2) model is significantly more accurate than the naive constant velocity / angle models.}
\label{fig:PredError}
\end{figure}

\section{Conclusion}
\begin{itemize}
\item Dynamic AR 2 models forecasting future acceleration and angles drastically improves forecasts of the future trajectory compared to the competing models.
\item Treating vehicles as heterogenous allows behaviour specific to an individual driver to be learned, improving logscores and forecast uncertainty.
\item This requires constant models fits and updates, which can be made computationally feasible by updating previous Variational Bayes model fits, which incurs only a small performance hit relative to the more expensive standard VB and MCMC methods.
\end{itemize}


\end{document}