\documentclass[12pt,a4paper]{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\usepackage{alltt}%
\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)

%2multibyte Version: 5.50.0.2960 CodePage: 1252
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[centertags]{amsmath}
\usepackage{graphicx}%
\usepackage{natbib}
\usepackage{color}
\usepackage[dvipsnames,svgnames*]{xcolor}
\usepackage{array}
\usepackage[hidelinks]{hyperref}
\usepackage[font=small,skip=5pt]{caption}
\usepackage[aboveskip=2pt]{subcaption}
\usepackage{amsmath}
\usepackage{amsthm}
%\usepackage{tikz}
%\usetikzlibrary{bayesnet}
\usepackage{url}
\usepackage{ulem}
\usepackage{afterpage}
\setcounter{MaxMatrixCols}{30}

\setlength{\topmargin}{0in}
\setlength{\oddsidemargin}{0.1in}
\setlength{\evensidemargin}{0.1in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8.25in}
\setlength\parindent{0pt}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\section{Introduction}

\begin{itemize}
\item Self driving cars becoming more common
\item Cars are easy capable of driving by themselves if there is no traffic around them
\item Cars need to be able to predict movement of other cars
\item Wide range of unconditional variances and first two partial autocorrelations observed while fitting each model individually - evidence of heterogeneity
\item Drivers display differing behaviour, could be able to group drivers into different categories
\item Heteogeneity between and within groups
\item Need to deal with new cars as they are observed
\end{itemize}

\section{Data processing}

\begin{itemize}
\item Modern cars are able to track lane marking and can calculate distance to other cars in relative co-ordinates (Thuy and Leon 2010)
\item Data from NGSIM is in coordinates relative to start of road - no impact of the curvature of the lane
\item Fit curve to car data to estimate lane midpoints (Woo et. al. 2016) 
\begin{enumerate}
\item At each point in time $t$ calculate the total distance each car has travelled along the road using $d_{i, t} = \sum_{s=1}^t v_{i, s}$, where $v_{i, s}$ is the velocity of car $i$ at time $s$. 
\item Estimate smoothing splines of the form $x_{i, t} = f(d_{i, t})$ and $y_{i, t} = g(d_{i, t})$, where $x_{i, t}$ and $y_{i, t}$ are the car coordinates relative to the start of the road.
\item Use polynomials to get $\hat{x}_{i, t}$ and $\hat{y}_{i, t}$ as estimates of the midpoint of the lane after travelling a distance $d_{i, t}$.
\item Define new coordinates 
\begin{align}
x^*_{i, t} &= \mbox{sign}(x_{i, t} - \hat{x}_{i, t})\sqrt{(x_{i, t}-\hat{x}_{i, t})^2 + (y_{i, t} - \hat{y}_{i, t})^2)} \label{xRel} \\
y^*_{i, t} &= d_{i, t} \label{yRel}
\end{align}
\end{enumerate}
\item Note: This should be calculated separately for each lane, cars used to estimate splines are excluded from further modelling
\end{itemize}

\section{Hierarchical Motion Model}

The position of car $i$ at time $t$ can be fully determined by its inital position, $\{x^*_{i, 0}, y^*_{i, 0}\}$ and the history of the driver's inputs: the cars velocity $v_{i, t}$ and steering angle, $\delta_{i, t}$ by
\begin{align}
x^*_{t} &= x^*_{t-1} + v_{t} \cos(\delta_{t}) \label{xEq} \\
y^*_{t} &= y^*_{t-1} + v_{t} \sin(\delta_{t}). \label{yEq},
\end{align}
where $\delta_{t} = \pi/2$ denotes that the car has no lateral movement and $x^*_{t} = x^*_{t-1}$.
From this relationship, the key inputs can be extracted by
\begin{align}
\delta_{t} &= \tan^{-1}\left(\frac{(y^*_{t} - y^*_{t-1})}{(x^*_{t} - x^*_{t-1})} \right) \label{dEq} \\
v_{t} &= \sqrt{(x^*_{t} - x^*_{t-1})^2 + (y^*_{t} - y^*_{t-1})^2} \label{vEq} \\
a_{t} &= v_{t} - v_{t-1}. \label{aEq}
\end{align}
where $a_{t}$ denotes the acceleration at time $t$.

Auto-regressive processes of orders $p$ and $q$ for $a_i$ and  $\delta_i$ are used to model the level of persistance and variation of driver actions, with
\begin{align}
a_{t} &= \sum_{j = 1}^p \phi_{j} a_{t-j} + \sigma_{\epsilon} \epsilon_{t} \label{aAR} \\
\delta_{t} &= \pi/2 + \sum_{j = 1}^q \gamma_{j} (\delta_{t-j} - \pi/2) + \sigma_{\eta} \eta_{t} \label{dAR}
\end{align}
where $\epsilon_{t}$ and $\eta_t$ are independently and identically distributed according to a standard normal distribution. The parameter vector to be estimated is collected as $\theta = \{\phi_{1}, \dots, \phi_{p}, \gamma_{1}, \dots, \gamma_{q}, \log(\sigma^{2}_{\epsilon}), \log(\sigma^{2}_{\eta})\} \in \mathbb{R}^{p + q + 2}$.

Multiple groups of differing driver behaviour can be allowed while still retaining heterogeneity with each group by jointly estimating each vehicle driver's individual parameter vector, $\theta_i$, as part of a heirarchical model, independently drawing $\theta_i$ from a $K$ component mixture of multivariate normal distributions, augmenting each $\theta_i$ with a mixture component $k_i = 1, \dots, K$ such that 
\begin{equation}
\theta_i | k_i = j \sim N(\mu_j, \Sigma_j).
\end{equation}
For each $j = 1, \dots, K$, multivariate normal priors are chosen for $\mu_j$ with mean $\bar{\mu}_j$ and variance matrix $\Omega_j$, Inverse Wishart priors are chosen for $\Sigma_j$ with degrees of freedom $\tau_j$ and scale matrix $\Psi_j$. $k_{i}$ has a multinomial distribution with mixture probabilities $\boldsymbol{\pi_i} = \{\pi_{1, i}, \dots, \pi_{K, i}\}$, where $\boldsymbol{\pi_i}$ has a Dirichlet prior with parameters $\alpha_1, \alpha_2, \dots, \alpha_K$. Defining $\beta = \{\mu_i, \Sigma_i, i = 1, \dots, K\}$, $y_{i, 1:T} = \{\delta_{i, s}, a_{i, s} | s = 1, \dots, T\}$ and $y_{1:N} = \{y_{i, 1:T} | i = 1, \dots, N\}$ the posterior distribution $p(\theta_1, \dots, \theta_N, \beta | y_{1:N})$ can be estimated with MCMC methods.


\begin{itemize}
\item To do: Add a Direchlet Process Prior to make this an infinite mixture
\item I've been trying to do this but I get results where all of the component indicator variables in the MCMC go to one component instead of the mixture assigning as many clusters as possible as Tas suggested would happen. I assume I'm doing something wrong.
\end{itemize}

\section{Streaming Data Process}

Notation:
\begin{itemize}
\item $q(\theta_i | y_{i, 1:T}) \approx p(\theta_i | y_{i, 1:T}, y_{1:N})$, $\lambda$ parameters and dependence on other cars suppressed - more important to be clear about which observations from the new vehicle are used to construct the approximation.
\end{itemize}

\begin{itemize}
\item While on the road a self driving car may wish to make posterior inferences on the parameter set of cars around them. They may be interested in the degree of erraticness / variance of their actions.
\item The posterior for car $N+1$ is given by 
\begin{equation}
p(\theta_{N+1} | y_{1:N+1}) \propto p(y_{N+1} | \theta_{N+1}) p(\theta_{N+1} | \beta) p (\beta | y_{1:N})
\end{equation}
\item As N is large replace $ p(\theta_{N+1} | \beta) p (\beta | y_{1:N})$ with $ p(\theta_{N+1} | \beta_{MAP})$ where $\beta_{MAP}$ is the maximum of $p(\beta | y_{1:N})$, or some other suitable point estimate. This approximate distribution now has a closed form. 
\item Subsequently fitting $p(\theta_{N+1} | y_{1:N}, y_{N+1, 1:S})  \propto p(y_{N+1} | \theta_{N+1}) p(\theta_{N+1} | \beta_{MAP})$ is fairly easy with MCMC / VB etc.
\item When $S$ is small, the information in the prior will dominate the individual information.
\item However, for some $T > S$ the information in the car's individual movements should become more relevant and forecasts could be improved by updating the distribution to $p(\theta_{N+1} | y_{1:N}, y_{N+1, 1:T})$.
\item This update takes the form of
\begin{equation}
p(\theta_{N+1} | y_{1:N}, y_{N+1, 1:T}) \propto p(y_{N+1, S+1:T} | \theta_{N+1}) p(\theta_{N+1} | y_{1:N}, y_{N+1, 1:S})
\end{equation}
\item Due to the lack of a closed form expression for $p(\theta_{N+1} | y_{1:N}, y_{N+1, 1:S})$, in reality the updating begins again from the prior re-using the data in $y_{N+1, 1:S}$.
\item Due to the demands of driving, the car wants the update carried out as soon as possible, and ideally should be able to discard this previously used information and use only $y_{N+1, S+1:T}$, reducing the time and memory complexity from $O(T)$ to $O(T-S)$. 
\item Defining $q(\theta_{N+1} | y_{N+1, 1:S})$ as the Variational Bayes approximation of $p(\theta_{N+1} | y_{1:N}, y_{N+1, 1:S})$, an online method is proposed to update $q(\theta_{N+1} | y_{N+1, 1:S})$ to $q(\theta_{N+1} | y_{N+1, 1:T})$:
\begin{enumerate}
\item Observe $y_{N+1, 1:S_1}$, fit VB $q(\theta_{N+1} | y_{N+1, 1:S_1}) \approx p(\theta_{N+1} | y_{1:N}, y_{N+1, 1:S_1})$
\item Observe $y_{N+1, S_1+1:S_2}$, in this case the model becomes
\begin{equation}
p(\theta_{N+1} | y_{1:N}, y_{N+1, 1:S_2}) \propto p(y_{N+1, S_1+1:S_2} | \theta_{N+1})p(\theta_{N+1} |  y_{1:N}, y_{N+1, 1:S_1})
\end{equation}
\item Set $p(\theta_{N+1} |  y_{1:N}, y_{N+1, 1:S_1}) = q(\theta_{N+1} | y_{N+1, 1:S_1})$
\item Fit VB  $q(\theta_{N+1} | y_{N+1, 1:S_2}) \approx p(\theta_{N+1} | y_{1:N}, y_{N+1, 1:S_2})$
\item Repeat as new data is observed
\end{enumerate}
\end{itemize}

\section{Empirical}

\begin{figure}[h]
\centering
\includegraphics[width = 0.8\textwidth]{HierSingleKDE.png}
\caption{Left: $p(\theta_i | \beta, y_{1:N})$ for a hierarchical mixture model with K = 6 and N = 2000. Right: Kernel Density Estimate of the posterior means from the same cars modelled individually. The hierarchical model has captured the bimodality and skewness present in the individual posterior means.}
\label{fig:HierSingleKDE}
\end{figure}

Models:
\begin{enumerate}
\item No prior information for the new car
\item Prior information from hierarchical model with one component
\item Prior information from large mixture hierarchical model
\end{enumerate}

Methods:
\begin{enumerate}
\item MCMC
\item Regular VB (fit to full sample)
\item Streaming VB (fit to new data only)
\end{enumerate}

Evaluation:
\begin{itemize}
\item Compare forecasts for different sample sizes and forecast horizons
\item Strategy: Fit model to one $S = 10$ (one second), forecast next second
\item Fit model to $S = 2$, forecast next second, repeat.
\item Collect logscores for predictive densities marginalised over the posterior distributions.
\end{itemize}

Results:
\begin{itemize}
\item Models based on the stromg prior information to outperform no information models for first few seconds, where we see a significant improvement in the log scores. As the dataset increases to > 100 observations (10 seconds), the data dominates and the prior choice does not have much impact. The median logscore (and probably other quantiles, hard to see from the graph) at a larger value of T increases as expected, showing that it is important to re-estimate the model with an increased dataset. However it seems that this re-estimation could probably be done with a simpler prior structure than the full hierarchy. 
\item Compare streaming data VB - Hoping it will be comparable to other methods, it technically may be a bit worse as it uses one more approximation than the standard VB, but I don't think this will have a very big impact. It certainly will be faster, so if the performance is roughly equal this is evidence in favour of the online method. 
\item Compare VB and MCMC methods for the same models to get an idea of approximation error.
\item Still waiting on the cluster for standard VB Finite Mixture Model and all VB streams.
\end{itemize}

\section{Conclusion}

\end{document}