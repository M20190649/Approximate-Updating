  \documentclass[12pt,a4paper]{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\usepackage{alltt}%
\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)

%2multibyte Version: 5.50.0.2960 CodePage: 1252
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[centertags]{amsmath}
\usepackage{graphicx}%
\usepackage{natbib}
\usepackage{color}
\usepackage[dvipsnames,svgnames*]{xcolor}
\usepackage{array}
\usepackage[hidelinks]{hyperref}
\usepackage[font=small,skip=5pt]{caption}
\usepackage[aboveskip=2pt]{subcaption}
\usepackage{amsmath}
\usepackage{amsthm}
%\usepackage{tikz}
%\usetikzlibrary{bayesnet}
\usepackage{url}
\usepackage{ulem}
\usepackage{afterpage}
\setcounter{MaxMatrixCols}{30}

\setlength{\topmargin}{0in}
\setlength{\oddsidemargin}{0.1in}
\setlength{\evensidemargin}{0.1in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8.25in}
\setlength\parindent{0pt}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\section{Introduction}

\begin{itemize}
\item Self driving Cars are becoming more common, eg Waymo, Tesla, GM.
\item The naviation systems used to drive involve predicting the movement of the surrounding traffic to find the safest path for the Self Driving Car.
\item These models use deep neural networks (DNN) to extract the properties of surrounding vehicles from sensors (eg. Cameras and LiDAR), such as their position, velocity, and steering angle (Woo et al. 2016a; Tian, Pei, Jana, and Ray 2017).
\item The Self Driving Car forecasts point estimates of the future trajectory of these surrounding vehicles based on the DNN results.
\item These can assume the surrounding vehicles will maintain their current angle and velocity (Gindele, Brechtel, Dillmann 2010; Waymo Safety Report 2017; Bautista 2017), but interest has developed in more advanced predictive models such as Neural Networks, Hidden Markov Models, and Support Vector Machines (Woo et al. 201b6; Tomar, Verma, and Tomar 2014; Woo et al. 2017; Geng et al. 2017).
\item This paper takes a statistical approach, fitting time series models to two extracted variables, velocity and steering angle, of vehicles in the Next Generation Simulation (NGSIM) US Highway 101 Dataset.
\item It then uses Bayesian methods to forecast the distribution of the future velocity and steering angle, which can be transformed into a trajectory forecast.
\item These time series models (In this case a bivariate AR2) fit a parameter vector to capture the driver behaviour, which can be very different from driver to driver (Chen 2017).
\item A hierarchical model is employed to allow heterogeneity between different drivers, where each vehicle is given its own parameter vector with a shared prior distribution.
\item This model implies that any new vehicle observed by the self driving car has its own parameter vector, related to previously observed vehicles through the shared prior.
\item The self driving car is constantly observing additional information about the vehicles around it. Forecasts where the parameter vector is conditioned on this additional data should outperform forecasts conditioned on less data.
\item A three step strategy is employed to forecast the future trajectory of newly observed vehicles:
\begin{enumerate}
\item Fit a hierarchical model with a large amount of vehicles to obtain the posterior distribution of each vehicles individual parameter vector and the shared distribution. This can occur before the self driving car is on the road.
\item When a new vehicle is observed use the information in this shared distribution to model the associated new parameter vector. 
\item Update the parameter vector and all forecasts at short intervals.
\end{enumerate}
\item This paper introduces a Variational Bayes updating method for the third step, where the posterior distribution of the new vehicles parameter vector is approximated, then this approximation is updated.
\item This only uses the data observed in-between updates, allowing the previously observed history of that vehicle to be discarded.
\item This is important for two reasons:
\begin{enumerate}
\item The model can be updated faster as less data must be processed, so forecasts can be made using as much recent data as possible.
\item Self driving cars take in up to 1GB of data a second (several websites report this as quotes from different people eg. Tech CEO, but I don't know if that's good enough for an academic citaiton), so discarding old data is useful.
\end{enumerate}
\item Compare Bayes MAP point estimates to a neural network and constant acceleration / angle model.
\item Error Metric: Euclidean Distance from predicted position to actual position
\item Compare log scores from streaming and standard VB forecast densities to see how the updating method compares to standard VB. I am also using MCMC, so we can compare either VB method to this, theoretically MCMC > VB Standard > VB Updating
\end{itemize}

\section{Data processing}

\begin{itemize}
\item Data is provided by the Next Generation Simulation (NGSIM) project, which observes ~5000 cars for a 2000 feet section of the US 101 Highway.
\item Observations of x/y coordinates are provided every 100 ms (10 hz), with around 500-1000 observations per car.
\item Around 2000 of these cars changed lanes or stopped moving somewhere in the sample and are discarded.
\item There was some curvature in the road which means there are systematic movements in the car paths.
\item Modern cars are able to track lane marking and can calculate distance to other cars in relative co-ordinates (Thuy and Leon 2010), instead of relative to the Earth (ie. ignores curves)
\item Prior research fits a polynomial curve to detected lane markings to build a model of the road lane edges (Woo et. al. 2016) 
\item Instead, I fit splines to the positions of each car to build an estimate of the road midpoint. Relative co-ordinates are found separately for each lane using the movements of 100 cars / lane by the following procedure:
\begin{enumerate}
\item At each point in time $t$ calculate the total distance each car has travelled along the road using $d_{i, t} = \sum_{s=1}^t v_{i, s}$, where $v_{i, s}$ is the velocity of car $i$ at time $s$. 
\item Estimate independent smoothing splines of the form $x_{i, t} = f(d_{i, t})$ and $y_{i, t} = g(d_{i, t})$, where $x_{i, t}$ and $y_{i, t}$ are the car coordinates relative to the start of the road.
\item Use models to get $\hat{x}_{i, t}$ and $\hat{y}_{i, t}$ as estimates of the midpoint of the lane after travelling a distance $d_{i, t}$.
\item Define new coordinates 
\begin{align}
x^*_{i, t} &= \mbox{sign}\left(\tan^{-1}\left(\frac{\hat{y}_{i, t} - y_{i, t}}{\hat{x}_{i, t} - x_{i, t}} \right) - \tan^{-1}\left(\frac{g'(d_{i, t}) }{f'(d_{i, t})}\right)\right)\sqrt{(x_{i, t}-\hat{x}_{i, t})^2 + (y_{i, t} - \hat{y}_{i, t})^2)} \label{xRel} \\
y^*_{i, t} &= d_{i, t} \label{yRel}
\end{align}
\end{enumerate}
\item (\ref{xRel}) \textit{really} needs a diagram.
\item These cars are not used for any further modelling, leaving 2873 cars travelling on a straightened out road with no noticable systematic curvature.
\end{itemize}

\section{Hierarchical Motion Model}

The position of any vehicle at time $t$ can be fully determined by its initial position, $\{x^*_{i, 0}, y^*_{i, 0}\}$ and the history of the driver's inputs: the vehicle's velocity $v_{i, t}$ and steering angle, $\delta_{i, t}$ by
\begin{align}
x^*_{t} &= x^*_{t-1} + v_{t} \cos(\delta_{t}) \label{xEq} \\
y^*_{t} &= y^*_{t-1} + v_{t} \sin(\delta_{t}). \label{yEq},
\end{align}
where $\delta_{t} = \pi/2$ denotes that the car has no lateral relative movement and $x^*_{t} = x^*_{t-1}$.
From this relationship, the driver's key inputs to motion can be extracted by
\begin{align}
\delta_{t} &= \tan^{-1}\left(\frac{(y^*_{t} - y^*_{t-1})}{(x^*_{t} - x^*_{t-1})} \right) \label{dEq} \\
v_{t} &= \sqrt{(x^*_{t} - x^*_{t-1})^2 + (y^*_{t} - y^*_{t-1})^2} \label{vEq} \\
a_{t} &= v_{t} - v_{t-1}. \label{aEq}
\end{align}
where $a_{t}$ denotes the acceleration at time $t$.

Auto-regressive processes of orders $p$ and $q$ for $a_t$ and  $\delta_t$ are used to model the level of persistance and variation of driver actions, with
\begin{align}
a_{t} &= \sum_{j = 1}^p \phi_{j} a_{t-j} + \sigma_{\epsilon} \epsilon_{t} \label{aAR} \\
\delta_{t} &= \pi/2 + \sum_{j = 1}^q \gamma_{j} (\delta_{t-j} - \pi/2) + \sigma_{\eta} \eta_{t} \label{dAR}
\end{align}
where $\epsilon_{t}$ and $\eta_t$ are independently and identically distributed according to a standard normal distribution. The unconditional means of $a_t$ and $\delta_t$ are set to zero and $\pi/2$ respectively, corresponding to forward motion at constant velocity. The parameter vector to be estimated is collected as 
\begin{equation*}
\label{thetaVec}
\theta = \{\phi_{1}, \dots, \phi_{p}, \gamma_{1}, \dots, \gamma_{q}, \log(\sigma^{2}_{\epsilon}), \log(\sigma^{2}_{\eta})\} \in \mathbb{R}^{p + q + 2}.
\end{equation*}

Multiple groups of differing driver behaviour can be allowed while still retaining heterogeneity within each group by jointly estimating each vehicle driver's individual parameter vector, $\theta_i$, as part of a heirarchical model. In this case each $\theta_i$ is treated as an independent draw from a $K$ component mixture of multivariate normal distributions, augmenting each $\theta_i$ with a mixture component $k_i = 1, \dots, K$ such that 
\begin{equation}
\theta_i | k_i = j \sim N(\mu_j, \Sigma_j).
\end{equation}
For each $j = 1, \dots, K$, multivariate normal priors are chosen for $\mu_j$ with fixed mean $\bar{\mu}_j$ and variance matrix $\Omega_j$, Inverse Wishart priors are chosen for $\Sigma_j$ with fixed degrees of freedom $\tau_j$ and scale matrix $\Psi_j$. $k_{i}$ has a multinomial distribution with mixture probabilities $\boldsymbol{\pi_i} = \{\pi_{1, i}, \dots, \pi_{K, i}\}$, where $\boldsymbol{\pi_i}$ has a Dirichlet prior with fixed hyperparameters $\alpha_1, \alpha_2, \dots, \alpha_K$. Defining $\beta = \{\mu_i, \Sigma_i, i = 1, \dots, K\}$, $z_{i, 1:T} = \{x^*_{i, s}, y^*_{i, s} | s = 1, \dots, T\}$ and $z_{1:N} = \{z_{i, 1:T} | i = 1, \dots, N\}$ the posterior distribution $p(\theta_1, \dots, \theta_N, \beta | z_{1:N})$ can be estimated with MCMC methods.

\section{Variational Bayes}

\subsection{Background}

Variational Bayes (Jordan et al. 1999) approximates a distribution $p(\theta | z)$ with a family $q(\theta)$, choosing the member of that family that minimises $KL(q(\theta) || p(\theta | z)$, where
\begin{equation}
\label{KL-div}
KL(q(\theta) || p(\theta | z) = \int_{\theta} q(\theta) \left(\log(q(\theta)) - \log(p(\theta | z))\right) d\theta.
\end{equation}
As $p(\theta | z)$ is typically intractable, VB maximises $\mathcal{L}(q)$ where
\begin{equation}
\label{ELBO}
\mathcal{L}(q, \lambda) = \int_{\theta} q(\theta) \left(\log(p(\theta, z)) - \log(q(\theta))\right) d\theta
\end{equation}
which is equivalent to minimising (\ref{KL-div}), where $\lambda$ is the parameter vector of $q(\theta)$.
This is done with stochastic gradient ascent, taking Monte Carlo estimates of $\delta\mathcal{L}(q, \lambda) / \delta \lambda$ (Hoffman et al. 2013)
and repeating updates of the form
\begin{equation}
\label{gradientAscent}
\lambda^{(m+1)} = \lambda^{(m)} + \rho^{(m)} \frac{\delta\mathcal{L}(q, \lambda)}{\delta \lambda} 
\end{equation}
which is guaranteed to converge to a local maximum (Robbins and Munro, 1951) if $\rho^{(m)}, m = 1, \dots, \infty$ satisfies
\begin{align}
&\sum_{m=1}^{\infty} \rho^{(m)} =  \infty \\
&\sum_{m=1}^{\infty} (\rho^{(m)})^2 <  \infty.
\end{align}
In this paper the sequence $\rho^{(m)}$ is provided by the Adam algorithm of Kingma and Ba (2015).

There are two Monte Carlo estimators of $\delta\mathcal{L}(q, \lambda) / \delta \lambda$, the Score Estimator of Ranganath, Gerrish and Blei (2014), 
\begin{equation}
\label{scoreDeriv}
\frac{\delta\mathcal{L}(q, \lambda)}{\delta \lambda} \approx \sum_{i = 1}^L \frac{\delta \log(q(\theta_i))}{\delta \lambda} \left(\log(p(\theta_i, z)) - \log(q(\theta_i)) \right), \mbox{ where } \theta_i \sim q(\theta),
\end{equation}
and the Reparameterised Estimator of Kingma Welling (2014),
\begin{equation}
\label{rpDeriv}
\frac{\delta\mathcal{L}(q, \lambda)}{\delta \lambda} \approx \sum_{i = 1}^M \frac{\delta f(\lambda, \epsilon_i)}{\delta \lambda} \frac{\delta \log(p(\theta, z))}{\delta \theta} \bigg\rvert_{\theta = f(\lambda, \epsilon_i)} + \frac{\delta J(\lambda, \epsilon_i)}{\delta \lambda}, \mbox{ where } \epsilon_i \sim r(\epsilon)
\end{equation}
where $r(\epsilon)$ is some distribution without any free parameters (eg. Uniform, Standard Normal), $f(\lambda, \epsilon)$ is some differentiable function that transforms $\epsilon$ to $\theta$, and $J(\lambda, \epsilon)$ is the Jacobian Matrix of this function. (\ref{rpDeriv}) typically has significantly lower variance than (\ref{scoreDeriv}), allowing $M$ to be set lower than $L$ which improves the speed of the algorithm; however the requirement for $f$ to exist restricts the class of approximating distributions $q(\theta)$ that can be used.
Notably, a differentiable $f(\lambda, \epsilon)$ does not exist when $q(\theta)$ is a mixture model and the derivative estimator (\ref{scoreDeriv}) must be used. To offset the increased variance and allow a reduction in $L$, following Gunawan, Tran, and Kohn (2017), Randomised Quasi Monte Carlo (RQMC) is used, where numbers in the unit hypercube are generated according to a Sobol Sequence (Sobol 1967), then these are randomised using the scrambled net method (Matousek 1998), and transformed into a draw of $\theta$ using the inverse-CDF of $q(\theta)$. Using RQMC allows (\ref{scoreDeriv}) to converge to the true derivative at a rate $O(L)$ compared to $O(\sqrt{L})$ in standard Monte Carlo. 

\subsection{Updating VB Process}

\begin{itemize}
\item While on the road a self driving car may wish to make posterior inferences on the parameter set of cars around them. They may be interested in the degree of erraticness / variance of their actions.
\item The posterior for car $N+1$ is given by 
\begin{equation}
\label{originalPost}
p(\theta_{N+1} z_{N+1} | z_{1:N}) \propto p(z_{N+1} | \theta_{N+1}) \int_{\beta} p(\theta_{N+1} | \beta) p (\beta | z_{1:N}) d\beta
\end{equation}
\item As N is large $p (\beta | z_{1:N})$ is fairly precise, so we can replace $\int_{\beta} p(\theta_{N+1} | \beta) p (\beta | z_{1:N}) d\beta$ with $ p(\theta_{N+1} | \hat{\beta})$ where $\hat{\beta}$ is the posterior mean of $p(\beta | z_{1:N})$. This approximation gives us a closed form for $ p(\theta_{N+1} | \hat{\beta})$.
\item Subsequently fitting $p(\theta_{N+1} | z_{1:N}, z_{N+1, 1:S})  \propto p(z_{N+1} | \theta_{N+1}) p(\theta_{N+1} | \hat{\beta})$ is fairly easy with MCMC / VB etc.
\item When $S$ is small, the information in the prior will dominate the individual information.
\item However, for some $T > S$ the information in the car's individual movements should become more relevant and forecasts could be improved by updating the distribution to $p(\theta_{N+1} | z_{1:N}, z_{N+1, 1:T})$.
\item This updated posterior is given by
\begin{equation}
\label{updatePost}
p(\theta_{N+1} | z_{N+1, 1:T}, z_{1:N}) \propto p(z_{N+1, S+1:T} | \theta_{N+1})p(\theta_{N+1} | z_{1:N}, z_{N+1, 1:S})
\end{equation}
\item Both $\mathcal{L}(q)$ derivative estimators, (\ref{scoreDeriv}) and (\ref{rpDeriv}), need to evaluate \newline $\log(p(\theta_{N+1}, z_{N+1, S+1:T} | z_{1:N}, z_{N+1, 1:S}))$, which takes the form
\begin{align}
\log(p(\theta_{N+1}, z_{N+1, S+1:T} | z_{1:N}, z_{N+1, 1:S})) &= \log(p(z_{N+1, S+1:T} | \theta_{N+1})) \nonumber \\
&+ \log(p(\theta_{N+1} | z_{1:N}, z_{N+1, 1:S})) \label{updateEq}
\end{align}
\item To perform this update only using the data from $z_{N+1, S+1:T}$ we would need a closed form form expression for $p(\theta_{N+1} | z_{1:N}, z_{N+1, 1:S})$ to replace the role of the prior. Without this closed form we would need to re-fit the VB model using the original prior $p(\theta_{N+1} | \hat{\beta})$ and the whole observed sequence $z_{N+1, 1:T}$.
\item Due to the demands of driving, the car wants the update carried out as soon as possible, and ideally should be able to discard the previously used information $z_{N+1, 1:S}$. 
\item Defining $q(\theta_{N+1} | z_{N+1, 1:S})$ as the Variational Bayes approximation of $p(\theta_{N+1} | z_{1:N}, z_{N+1, 1:S})$, a new method is proposed to update $q(\theta_{N+1} | z_{N+1, 1:S})$ to $q(\theta_{N+1} | z_{N+1, 1:T})$ after observing $z_{N_1, S+1:T}$.
\item By replacing $p(\theta_{N+1} | z_{1:N}, z_{N+1, 1:S})$ in (\ref{updateEq}) with the previously fit $q(\theta_{N+1} | z_{N+1, 1:S})$ we get an approximation to (\ref{updateEq}):
\begin{equation}
\label{updateEqApprox}
\log(p(\theta_{N+1}, z_{N+1, 1:T} | z_{1:N})) \approx \log(p(z_{N+1, S+1:T} | \theta_{N+1})) + \log(q(\theta_{N+1} | z_{N+1, 1:S}))
\end{equation}
\item Substituting (\ref{updateEqApprox}) into (\ref{scoreDeriv}) gives the Score Estimator for updating Variational Bayes, 
\begin{align}
\label{scoreUpdate}
\frac{\delta\mathcal{L}(q, \lambda)}{\delta \lambda} \approx \sum_{i = 1}^L \frac{\delta \log(q(\theta_{N+1, i} | z_{N+1, 1:T}))}{\delta \lambda} &\left(\log(q(\theta_{N+1, i} | z_{N+1, 1:S}) - \log(q(\theta_{N+1} | z_{N+1, 1:T})) \right), \nonumber \\
& \mbox{ where } \theta_{N+1, i} \sim q(\theta_{N+1} | z_{N+1, 1:T}).
\end{align}
\item Similarly, substituting (\ref{updateEqApprox}) into (\ref{rpDeriv}) gives the Reparameterised Estimator for updating Variational Bayes,
\begin{equation}
\label{rpUpdate}
\frac{\delta\mathcal{L}(q, \lambda)}{\delta \lambda_T} \approx \sum_{i = 1}^M \frac{\delta f(\lambda_T, \epsilon_i)}{\delta \lambda} \frac{\delta \log(q(\theta_{N+1} | z_{N+1, 1:S}))}{\delta \theta_{N+1}} \bigg\rvert_{\theta_{N+1} = f(\lambda_T, \epsilon_i)} + \frac{\delta J(\lambda_T, \epsilon_i)}{\delta \lambda_T}, \mbox{ where } \epsilon_i \sim r(\epsilon)
\end{equation}
where $\lambda_T$ denotes the parameter vector of $q(\theta_{N+1} | z_{N+1, 1:T})$.
\item This process can be repeated as neccessary, and is $O(1)$ instead of $O(T)$ assuming $S$ grows at the same rate as $T$.
\end{itemize}

\section{Empirical}

\begin{figure}[ht]
\centering
\includegraphics[width = 0.8\textwidth]{HierSingleKDE.png}
\caption{Top: A Kernel Density Estimate of the distribution of posterior means for each $\theta_i | z_i, i = 1, \dots, 2000$ modelled independently with no hierarchical structure. Bottom: $p(\theta_i | \beta, z_{1:2000})$ for a hierarchical mixture model with $K = 6$ using the same vehicles, with each component of the mixture denoted by the coloured densities. The hierarchical model has captured the bimodality and skewness present in the independent posterior means.}
\label{fig:HierSingleKDE}
\end{figure}

Models:
\begin{enumerate}
\item No prior information for the new car - very flat priors.
\item Prior information from hierarchical model with one component
\item Prior information from large mixture hierarchical model
\end{enumerate}
\begin{itemize}
\item In each case the VB posterior approximation $q(\theta)$ has the same family as the prior $p(\theta)$, so either a multivariate normal or a mixture of multivariate normals. This is so we can replace the prior with the VB fit at each step of the updating method easier. 
\item The Hierarchical model is fit for 2000 cars, leaving 873 for forecast evaluation.
\end{itemize}

Methods:
\begin{enumerate}
\item MCMC
\item Regular VB (fit each update to the new sample)
\item Updating VB (fit to new data using the previous VB fit as a prior)
\end{enumerate}


\begin{center}
\begin{table}[h]
\resizebox{\textwidth}{!}{%
\begin{tabular}{| l | c | c | c |}
\hline
& Non-Informative Prior & Simple Hierarchy & Finite Mixture Hierarchy \\
\hline
MCMC & \multicolumn{3}{c|}{Random Walk Metropolis Hastings jointly drawing the entire $\theta$ vector using Garthwaite, Fan and Sisson (2015)} \\
& \multicolumn{3}{c|}{to control the variance matrix of the Multivariate Gaussian proposal distribution} \\
\hline
VB  & \multicolumn{2}{c|}{Variational Bayes using (\ref{rpDeriv}) to update parameters of a} & Variational Bayes using (\ref{scoreDeriv}) to update parameters of a six \\
Standard & \multicolumn{2}{c|}{Multivariate Gaussian approximation with non-zero covariance} & component mixture of diagonal variance Multivariate Gaussians\\
\hline
VB & \multicolumn{2}{c|}{Variational Bayes using (\ref{rpUpdate}) to update parameters of a} &  Variational Bayes using (\ref{scoreUpdate}) to update parameters of a six \\
Updating & \multicolumn{2}{c|}{Multivariate Gaussian approximation with non-zero covariance} & component mixture of diagonal variance Multivariate Gaussians\\ 
\hline
\end{tabular}}
\label{tableAlg}
\caption{Details of the algorithms used to produce posterior distributions for each method and prior combination}
\end{table}
\end{center}

There are six naive forecast models with constant estimates of the future values of $v_t$ and $\delta_t$ for $t = T+1, \dots, T+30$. $v_t$ is estimated by either the current velocity, $v_T$ or the average of the previous ten lags of $v$, $1/10 \sum_{s = T - 9}^T v_s$. $\delta_t$ is estimated by one of the current steering angle, $\delta_T$, the average of the previous ten lags of $\delta$, $1/10 \sum_{s = T - 9}^T \delta_s$ or by $\pi/2$, i.e. the car is moving directly ahead. Table \ref{tableNaive} classifies each naive model based on the combination of velocity and angle estimates.
\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
& $\hat{\delta}_t = \delta_T$ & $\hat{\delta}_t = 1/10 \sum_{s=T-9}^T \delta_s$ & $\hat{\delta}_t = \pi/2$ \\
\hline
$\hat{v}_t = v_T$ & Naive 1 & Naive 2 & Naive 3 \\
$\hat{v}_t = 1/10\sum_{s=T-9}^T$ & Naive 4 & Naive 5 & Naive 6\\
\hline
\end{tabular}
\end{center}
\label{tableNaive}
\caption{Classification of naive forecast models by future velocity and steering angle estimators}
\end{table}


Evaluation:
\begin{itemize}
\item Compare forecasts for different sample sizes and forecast horizons
\item Strategy: Fit model for one car using data up to time $S = 10$ (one second), forecast from $S+1:S+30$ (three seconds seems to be a standard forecast horizon for driver reaction, though surely self driving cars have a faster reaction time).
\item Fit model to $S = 20$, forecast next three seconds, repeat.
\item Collect logscores for predictive densities marginalised over the posterior distributions.
\item Use MAP of the predictive density to get a point estimate of the future position, compare to the constant velocity / angle model.
\item Repeat for each of the 873 remaining cars.
\end{itemize}

\begin{figure}[ht]
\centering
\includegraphics[width = 0.95\textwidth]{CarsBoxplots.png}
\caption{Predictive logscores for $p(z_{i, T+h} | z_{i, 1:T})$ for $h = 1, \dots, 30$ and $T = \{10, 20, \dots, 300\}$ for each of the three levels of prior information fit by MCMC, standard VB and updating VB. The median logscore across all time periods, forecast horizons, and priors for the updating VB algorithm is 3.28, compared to 3.43 for standard VB and 3.46 for MCMC.}
\label{fig:MainResults}
\end{figure}

\begin{center}
\begin{table}[h]
\begin{tabular}{| l | c | c | c |}
\hline
& Non-Informative Prior & Simple Hierarchy & Finite Mixture Hierarchy \\
\hline
MCMC & 3.42 & 3.47 & 3.51 \\
VB Standard & 3.35 & 3.46 & 3.48 \\
VB Updating & 3.14 & 3.28 & 3.46 \\
\hline
\end{tabular}
\label{tableMedian}
\caption{Median predictive logscore for $p(z_{i, T+h} | z_{i, 1:T})$ for $h = 1, \dots, 30$ and $T = \{10, 20, \dots, 300\}$ for each prior and algorithm combination}
\end{table}
\end{center}

\begin{center}
\begin{table}[h]
\begin{tabular}{| l | c | c | c |}
\hline
& Non-Informative Prior & Simple Hierarchy & Finite Mixture Hierarchy \\
\hline
MCMC & 0.81 & 0.77 & 0.77 \\
VB Standard & 0.79 & 0.77 & 0.77 \\
VB Updating & 0.79 & 0.78 & 0.77 \\
\hline
\end{tabular}
\label{tableError}
\caption{Mean predictive Euclidean error in feet for $p(z_{i, T+h} | z_{i, 1:T})$ for $h = 1, \dots, 30$ and $T = \{10, 20, \dots, 300\}$ for each prior and algorithm combination. The mean predictive Euclidean error for the constant velocity / angle model is 3.26 feet.}
\end{table}
\end{center}

\begin{figure}[ht]
\centering
\includegraphics[width = 0.95\textwidth]{PredictiveError.png}
\caption{Mean Euclidian Error for car position predictions at a forecast horizon of one second, two seconds or three seconds for the Finite Mixture prior / VB Update Algorithm compared to the constant velocity and angle model. The average error at the three second forecast horizon is reduced from 7.16 feet to just 0.89 feet.}
\label{fig:MainResults}
\end{figure}

\section{Conclusion}
\begin{itemize}
\item Fitting individual AR 2 models on each driver's velocity and steering angle drastically improves forecasts of the future trajectory compared to the constant model used in practice
\item Strong prior information improves forecasts early on in the sample, but has a reduced impact after ~20 seconds of observation.
\item Constantly fitting individual models in this way can be made computationally feasible by updating previous Variational Bayes model fits, which incurs only a small performance hit relative to the more expensive standard VB and MCMC methods.
\end{itemize}


\end{document}