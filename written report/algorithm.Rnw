\documentclass{article}
\usepackage{amsmath}
\usepackage[]{algorithm2e}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\begin{document}
%\SweaveOpts{concordance=TRUE}

\begin{algorithm}[H]
 \SetKwInOut{Input}{Log Joint Density}
 \KwResult{Mean Field Approximation}
 Initialise $\lambda$ randomly\;
 \While{Not converged}{
  \For{$i =  1$ \KwTo $7$}{
      Hold $\lambda_{j \neq i}$ fixed\;
      Maximise $\lambda_{i}$ \;
     }
 }
 \caption{Mean Field Variational Bayes}
\end{algorithm}

\begin{eqnarray}
\nabla_{\lambda} L(\theta, \lambda) & = & E_{q} \left[ \nabla_{\lambda}[q_{\theta}(\log(p(\theta, y)) - \log(q(\theta)))] \right] \\
& = & E_{q} \left[ \nabla_{\lambda}[\log(q_{\theta})](\log(p(\theta, y)) - \log(q(\theta))) \right] \nonumber \\
& \approx & 1/S \sum_{s=1}^{S} \nabla_{\lambda}[\log(q_{\theta^s})](\log(p(\theta^s, y)) - \log(q(\theta^s))) \nonumber
\end{eqnarray}

\begin{algorithm}[H]
 \SetKwInOut{Input}{Log Joint Density, Approximation family q}
 \KwResult{Variational Approximation}
 Initialise $\lambda$ to Methods of Moments Estimator\;
 \While{Not converged}{
  Simulate $\theta_s$ for $s = 1, \dots S$ from $q(\theta^{t-1})$
  \For{$i =  1$ \KwTo $7$}{
      Hold $\lambda_{j \neq i}$ fixed\;
      Calculate $\nabla_{\lambda_i}^{t} =  1/S\sum_{s=1}^{S} \nabla_{\lambda}[\log(q_{\theta^s})](\log(p(\theta^s, y)) - \log(q(\theta^s)))$
     }
  Set $\lambda^{t} = \lambda^{t-1} + p_t \nabla_{\lambda}^{t}$
  Set $t = t + 1$
 }
 \caption{Stochastic Gradient Ascent Algorithm 1}
\end{algorithm}

We can use $(\theta_1, \theta_2)' = \mu + L (\epsilon_1, \epsilon_2)'$ and $\theta_3 = Q^{-1}(\epsilon_3 | \alpha, \beta)$ where $(\epsilon_1, \epsilon_2) \sim N(0, I)$ and $\epsilon_3 \sim U(0, 1)$


\begin{algorithm}[H]
 \SetKwInOut{Input}{Log Joint Density, Approximation family q}
 \KwResult{Variational Approximation}
 Initialise $\lambda$ to Methods of Moments Estimator\;
 \While{Not converged}{
  Simulate $\epsilon$ for $s = 1, \dots S$ from $N(0, I)$ and $U(0, 1)$
  Transform $\theta^s = f(\epsilon^s, \lambda^{t-1})$
  \For{$i =  1$ \KwTo $7$}{
      Hold $\lambda_{j \neq i}$ fixed\;
      Calculate $\nabla_{\lambda_i}^{t} =  1/S\sum_{s=1}^{S} \nabla_{\lambda}[(\log(p(\theta^s, y)) - \log(q(\theta^s)))]$
     }
  Set $\lambda^{t} = \lambda^{t-1} + p_t \nabla_{\lambda}^{t}$
  Set $t = t + 1$
 }
 \caption{Stochastic Gradient Ascent Algorithm 2}
\end{algorithm}

<<Echo = FALSE, Eval = TRUE>>=
library(knitr)
library(mvtnorm)
library(reshape)
summary = matrix( c(0.37, -0.30, 0.10, -0.03, 0.10, 49.2, 78.4, -77.5, 7.4,
                    0.37, -0.29, 0.09, 0, 0.10, 49.5, 79.8, -77.4, "1.3 x 10^{-6}", 
                    0.32, -0.32, 0.11, -0.01, 0.12, 46.4, 78.7, -77.2, 59.5, 
                    0.38, -0.30, -0.1, 0.03, -0.10, 48.7, 78.9, -77.4, 1.2) , byrow = TRUE, ncol = 9)
                    
colnames(summary) = c(paste0(expression(mu), 1), paste0(expression(mu), 2), "L11", "L21", "L22", expression(alpha), expression(beta), "L(theta)", "time (seconds)*")
rownames(summary) = c("Method of Moments", "Mean Field", "Copula 1", "Copula 2")
kable(summary, format = "latex")
@

\end{document}