\documentclass{article}
\usepackage{amsmath}
\usepackage[]{algorithm2e}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\begin{document}

\tableofcontents

\section{Bayesian Inference}
\subsection{Bayes Rule}
-General Intro to Bayesian Methodology
-Denominator as an integral
-Often no analytical solutions
\subsection{Exact Bayesian Computation}
-Numerical Integration
-Importance Sampling
  -Trade computation for more accuracy as needed
-MCMC
  -Gibbs
  -Metropolis Hastings
  -Hamiltonian MCMC


\section{Variational Bayes}
\subsection{Estimation as an Optimisation}
-KL Divergence as a transformation to distribution space
-VB Derivation
\subsection{The Mean Field Assumption}
-Mean Field Derivation
-Gives optimal family of approximating distributions
-Lots of analytical work
-Very Fast
\subsection{Stochastic Gradient Ascent}
-SGA Derivation
-Step Size
-Flexibility 
-Little analytical work
-Needs a pre-defined approximating distribution family
\subsection{Variance Reduction Techniques}
-Rao Blackwellisation
-Control Variates
-Reparameterisation
\subsection{AR2 model example}

\section{Approximate Forecast Updating}
\subsection{Motivation}
-Slow convergence of MCMC
-Bad scaling to large dimensional models and datasets
-Time/Computational Constraints
-Motivation from short forecast windows
\subsection{Model Selection}
-Overnight/Intermittent MCMC
-Infer model structure from draws
-Max KL by MLE - only for KL going the other way
\subsection{Posterior Updates}
-Distributions should not change much
-Start new optimisation at old maximum
\subsection{Forecasting}
-Have easy parametric form of posterior to work with

\end{document}