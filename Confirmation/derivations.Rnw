\documentclass{article}
\usepackage{natbib}
\usepackage{color}
\usepackage[dvipsnames,svgnames*]{xcolor}
\usepackage{array}
\usepackage[colorlinks=TRUE, linkcolor=blue]{hyperref}
\usepackage[font=small,skip=5pt]{caption}
\usepackage[aboveskip=2pt]{subcaption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{ulem}
\usepackage{afterpage}
\usepackage{bbm}


\begin{document}

\title{Derivations}

\section{Variational Bayes}

First we define the KL Divergence between distributions $q(\theta)$ and $p(\theta)$ as 

$$KL[q(\theta)|p(\theta)] = \int q(\theta) \ln \left( \frac{q(\theta)}{p(\theta)}\right) d\theta.$$

This is a non-negative function that takes on a value of $0$ if and only if $q(\theta) = p(\theta)$.

Then, using the unknown constant $\ln(p(y))$, we have

\begin{eqnarray}
\ln(p(y)) & = & \int_{\theta} q(\theta|\lambda) \ln(p(y)) d\theta \nonumber \\
& = & \int_{\theta} q(\theta|\lambda) \ln  \left( \frac{p(y, \theta)}{p(\theta | y)} \right) d\theta \nonumber \\
& = & \int_{\theta} q(\theta|\lambda) \ln  \left( \frac{p(y, \theta)}{p(\theta | y)} \frac{q(\theta|\lambda)}{q(\theta|\lambda)}\right) d\theta \nonumber \\
& = & \int_{\theta} q(\theta|\lambda) \ln  \left( \frac{q(\theta|\lambda)}{p(\theta | y)}\right) + \ln \left( \frac{p(y, \theta}{q(\theta|\lambda)} \right) d\theta \nonumber \\
& = & \int_{\theta} q(\theta|\lambda) \ln  \left( \frac{q(\theta|\lambda)}{p(\theta | y)}\right) d\theta + \int_{\theta} q(\theta|\lambda) \ln \left( \frac{p(y, \theta)}{q(\theta|\lambda)} \right) d\theta \nonumber \\
& = & KL [q(\theta|\lambda) || p(\theta | y)] + \mathcal{L}(q(\theta | \lambda), y) \nonumber
\end{eqnarray}

where 
\begin{eqnarray*}
\mathcal{L}(q(\theta|\lambda), y) & = & \int_{\theta} q(\theta|\lambda) \ln \left( \frac{p(y, \theta|\lambda)}{q(\theta|\lambda)} \right) d\theta \\
& = &  \int_{\theta} q(\theta|\lambda) \ln (p(y, \theta|\lambda)) d\theta -  \int_{\theta} q(\theta|\lambda) \ln (q(\theta|\lambda)) d\theta.
\end{eqnarray*}

Hence we have that
$$ KL [q(\theta|\lambda) || p(\theta | y)] = \ln(p(y)) - \mathcal{L}(q(\theta | \lambda), y)$$

and clearly maximising $\mathcal{L}(q(\theta | \lambda), y)$ with respect to $q(\theta|\lambda)$ will minimise the KL divergence between the unknown true posterior $p(\theta|y)$ and an approximating distribution $q(\theta|\lambda)$.

\section{The Mean Field Assumption}

If our approximating distribution $q(\theta|\lambda)$ factorises as

$$q(\theta|\lambda) = \prod_i q(\theta_i | \lambda_i)$$,

where each component $\theta_i$ may be a scalar or a vector, and using the notation $q_i = q(\theta_i|\lambda_i)$ and $q_{\setminus i} = \prod_{j\neq i}q_j$, we have:

\begin{eqnarray}
\label{mf}
\mathcal{L}(q(\theta | \lambda), y) & = &\int_{\theta} q_i q_{\setminus i} \ln (p(y, \theta)) d\theta - \int_{\theta} q_{i}q_{\setminus i} \ln (q_{i}q_{\setminus i}) d\theta \nonumber \\
& = & \int_{\theta_i}\int_{\theta_{\setminus i}} q_{i}q_{\setminus i} ( \ln (p(y, \theta)) - \ln(q_{i})) d\theta_i d\theta_{\setminus i} - \int_{\theta_i}\int_{\theta_{\setminus i}} q_{i}q_{\setminus i} \ln (q_{\setminus i}) d\theta_i d\theta_{\setminus i}  \nonumber \\
& = & \int_{\theta_{i}} q_{i} \left( \int_{\theta_{\setminus i}} q_{\setminus i} \ln (p(y, \theta )) d\theta_{\setminus i} - \ln(q_i) \right) d\theta_i \nonumber \\
&- & \int_{\theta_i} q_{i} \int_{\theta_{\setminus i}} q_{\setminus i} \ln(q_{\setminus i}) d\theta_{\setminus i} d\theta_i \nonumber \\
& = & \int_{\theta_{1}} q_{i} \ln \left( \frac{\exp(  \mathbb{E}_{q_{\setminus i}} [\ln(p(y,\theta))])}{q_i} \right) d\theta_i - \int_{\theta_{\setminus i}} q_{\setminus i} \ln(q_{\setminus i}) d\theta_{\setminus i} \\
& = & -KL (q_i || \exp( \mathbb{E}_{q_{\setminus i}} [\ln(p(y,\theta))])) + c \nonumber
\end{eqnarray}

as the rightmost component of (\ref{mf}) is constant with respect to $q_i$.

We can see that, holding $q_{\setminus_i}$ fixed, we can maximise $\mathcal{L}(q(\theta | \lambda))$ with respect to $q_i$ by minimising the term KL Divergence between $q_i$ and $\exp( \mathbb{E}_{q_{\setminus i}} [\ln(p(y,\theta))])$ by setting

\begin{equation}
\label{mf2}
q(\theta_i | \lambda_i) \propto\exp( \mathbb{E}_{q_{\setminus i}} [\ln(p(y,\theta))]).
\end{equation}

The distributional family for any $q_i$ in (\ref{mf2}) that minimises the KL Divergence between the true posterior and the appoximation is found by taking the log-joint density, ignoring any term that doesn't depend on $\theta_i$, taking expectations of any remaining $\theta_{j \neq i}$ and then exponentiating. These steps are the same that can be used to find a conditional distribution $p(\theta_i | \theta_{j \neq i})$, with dependence on other parameters replaced by their expectations. Hence, the distributional family for any $\theta_i$ in Mean-Field Variational Bayes is the exact same distributional family as $p(\theta_i | \theta_{j \neq i})$ as used in Gibbs MCMC schemes.
\vspace{5mm}

Given these distributional families, an algorithm that cycles through the $\lambda_i$ and sets each to the value given by (\ref{mf2}) is known as a coordiante ascent algorithm. Typically the expectations make each $\lambda_i$ a function of $\lambda_{j \neq i}$ and the algorithm must continuously iterate between each parameter until $\mathcal{L}(q(\theta | \lambda), y)$ converges with some pre-defined threshold.

\section{Stochastic Gradient Ascent}

An alternative method to maximise the function $\mathcal{L}(q(\theta | \lambda), y)$ is through a gradient ascent algorithm, where, for $t = 1, 2, \dots$,

$$\lambda_t = \lambda_{t-1} + \rho_t \nabla_{\lambda} \mathcal{L}(q(\theta | \lambda_{t-1}), y),$$

given some initial value $\lambda_0$ for some sequence $\rho_t$ that satisfies the Robbins-Monro conditions,

\begin{eqnarray*}
\sum_{t=1}^{\infty} \rho_t & = & \infty \\
\sum_{t=1}^{\infty} \rho^2_t & < & \infty.
\end{eqnarray*}

This algorithm does not require $q(\theta | \lambda)$ to factorise and so can be applied to distributions $q(\theta | \lambda)$ that allow parameter dependence. However, it does not provide the family of the the optimal approximating distribution, unlike under the mean field assumption. Restricting ourselves by the assumption that we can interchange the order of differentation with respect to $\lambda$ and integration with respect to $\theta$, we can calculate derivatives of $\mathcal{L}(q(\theta | \lambda), y)$ as follows:

\begin{align}
\nabla_{\lambda} \mathcal{L}(q(\theta | \lambda), y) &=  \nabla_{\lambda} \int_{\theta} q(\theta | \lambda) \left( \ln (p(y, \theta)) - \ln(q(\theta | \lambda)) \right) d\theta \nonumber \\
&=   \int_{\theta} \nabla_{\lambda}[  q(\theta | \lambda) \left( \ln (p(y, \theta)) - \ln(q(\theta | \lambda)) \right)] d\theta \label{grad1} \\
&=  \int_{\theta}  q(\theta | \lambda) \nabla_{\lambda}[( \ln (p(y, \theta)) - \ln(q(\theta | \lambda))] d\theta \nonumber \\ 
&+ \int_{\theta} \nabla_{\lambda} [q(\theta | \lambda)] \left( \ln (p(y, \theta)) - \ln(q(\theta | \lambda)) \right) d\theta \label{grad2} \\
&=  \int_{\theta} \nabla_{\lambda} [q(\theta | \lambda)] \left( \ln (p(y, \theta)) - \ln(q(\theta | \lambda)) \right) d\theta \nonumber \\
&=   \int_{\theta} \nabla_{\lambda} [\ln(q(\theta | \lambda))]\left( \ln (p(y, \theta)) - \ln(q(\theta | \lambda)) \right)q(\theta | \lambda) d\theta \nonumber \\
&= E_q [  \nabla_{\lambda} [\ln(q(\theta | \lambda))]\left( \ln (p(y, \theta)) - \ln(q(\theta | \lambda)) \right)] \label{grad3}
\end{align}

Note that the first integral on the right hand side of (\ref{grad2}) equals 0 as

\begin{eqnarray*}
\int_{\theta}  q(\theta | \lambda) \nabla_{\lambda}[( \ln (p(y, \theta))] d\theta & = & 0
\end{eqnarray*}

and

\begin{eqnarray*}
\int_{\theta} - q(\theta | \lambda) \nabla_{\lambda}[ \ln(q(\theta | \lambda))] d\theta & = & - \int_{\theta} \frac{\nabla_{\lambda}q(\theta | \lambda)}{q(\theta | \lambda)} q(\theta | \lambda) d\theta \\
& = & -  \nabla_{\lambda}\int_{\theta} q(\theta | \lambda) d\theta \\
& = & 0
\end{eqnarray*}

We can compute a Monte-Carlo estimate of (\ref{grad3}) by 

\begin{equation}
\label{mc}
\nabla_{\lambda}\mathcal{L}(q(\theta | \lambda)) \approx \frac{1}{S}\sum_{s=1}^{S} \nabla_{\lambda} [\ln(q(\theta_s | \lambda))] (\ln (p(y, \theta_s)) - \ln(q(\theta_s | \lambda)))
\end{equation}

with $\theta_s \sim q(\theta | \lambda)$.

As the distribution $q(\theta | \lambda)$ is specified by the user, the main restriction on the use of a stochastic gradient ascent algorithm is that the log-joint density $\ln(p(y, \theta))$ is able to be evaluated.

\section{Variance Reduction}

The Monte-Carlo estimate in (\ref{mc}) often has far too large a variance to useful without small step sizes $\rho_t$ and a large number of draws $S$. These significantly slow down the algorithm, and a lower variance estimator is more useful.

\subsection{Reparamatisation}

Consider a unit free distribution $q(\epsilon)$ and detetministic transform $f(\cdot,\cdot)$ such that $\theta = f(\epsilon, \lambda)$. Examples include a location-scale transformation from a standard gaussian or an inverse-CDF transform from a uniform$(0, 1)$ variable. Note that

$$q_\theta(\theta | \lambda) = q_\epsilon(\epsilon) \left| \frac{d\epsilon}{d\theta} \right| $$

where the parameters that govern the transform $f$ are the same $\lambda$ parameters as in $q(\theta | \lambda)$.

and (\ref{grad1}) becomes

\begin{align}
\label{grad4}
\nabla_{\lambda} \mathcal{L}(q(\theta | \lambda), y) &=  \int_{\epsilon} \nabla_{\lambda}[  q(\epsilon) \left( \ln (p(y, f(\epsilon, \lambda))) - \ln(q(f(\epsilon, \lambda) | \lambda) \right)] d\theta \nonumber \\
&=  \int_{\epsilon}  q(\epsilon) \nabla_{\lambda} \left[  ( \ln (p(y, f(\epsilon, \lambda))) - \ln(q(f(\epsilon, \lambda)| \lambda)) \right] d\theta \nonumber \\
&\approx  \frac{1}{S}\sum_{s=1}^{S} \nabla_{\lambda} \left[  ( \ln (p(y, f(\epsilon_s, \lambda))) - \ln(q(f(\epsilon_s, \lambda)| \lambda)) \right].
\end{align}

with $\theta_s \sim q(\epsilon)$.

This reparameterised version often has orders of magnitude lower variance than the estimator in (\ref{mc}).

\subsection{Control Variates}

If reparameterisation is too difficult or otherwise unavailable, we can use control variates to estimate our function $g$ with reduced variance. 

With

\begin{equation}
\label{grad5}
g(\theta, \lambda, y) =  \nabla_{\lambda} [\ln(q(\theta_s | \lambda))] (\ln (p(y, \theta_s)) - \ln(q(\theta_s | \lambda)))
\end{equation}

and for some choice of $h$, define

\begin{equation}
\label{controlv}
\hat{g}(\theta, \lambda, y) = g(\theta, \lambda, y) - a(h(\theta, \lambda, y) - \mathbb{E}(h(\theta, \lambda, y))).
\end{equation}

Then we have that

\begin{equation}
\label{controlv2}
Var(\hat{g}) = Var(g) + a^2 Var(h) - 2aCov(g, h).
\end{equation}

Solving the polynomial in $a$ shows that the variance of $\hat{g}$ is minimised by 

\begin{equation}
\label{controlv3}
\hat{a} = Cov(g, h)/Var(h).
\end{equation}

We need to choose some function $h$ that has a high covariance with (\ref{grad5}), so choose 

\begin{equation}
\label{controlv4}
h = \nabla_{\lambda} [\ln(q(\theta_s | \lambda))]
\end{equation}

As $h$ is a score function, it has an expectation of zero and our estimator becomes

\begin{equation}
\nabla_{\lambda} \mathcal{L}(q(\theta | \lambda), y) \approx \frac{1}{S}\sum_{s=1}^{S} \nabla_{\lambda} [\ln(q(\theta_s | \lambda))] (\ln (p(y, \theta_s)) - \ln(q(\theta_s | \lambda)) + \hat{a})
\end{equation}

with $\hat{a}$ estimated from the corresponding sample statistics from a subset of the $S$ Monte-Carlo draws.



\end{document}