\documentclass{article}
\usepackage{natbib}
\usepackage{color}
\usepackage[dvipsnames,svgnames*]{xcolor}
\usepackage{array}
\usepackage[colorlinks=TRUE, linkcolor=blue]{hyperref}
\usepackage[font=small,skip=5pt]{caption}
\usepackage[aboveskip=2pt]{subcaption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{ulem}
\usepackage{afterpage}
\usepackage{bbm}

\begin{document}

\section{The AR2 Model}

Consider a mean zero AR(2) model conditioned on $y_1$ and $y_2$.

$$y_t = \phi_1 * y_{t-1} + \phi_2 * y_{t-2} + \epsilon_t \mbox{ for } t = 3,\dots,T$$,

where $\epsilon$ are iid $\mathcal{N}(0, \sigma^2)$.

Using the independent priors: 

\begin{align}
p(\phi_1) &\sim \mathcal{N}(\bar{\phi_1}, \tau_{1}^2) \nonumber \\
p(\phi_2) &\sim \mathcal{N}(\bar{\phi_2}, \tau_{2}^2) \nonumber \\
p(\sigma^2) &\sim \mathcal{IG} (\mbox{shape} = \alpha, \mbox{ rate} = \beta),
\end{align}
we have a model with zero stationary restrictions that can be easily sampled through Gibbs MCMC or approximated by a range of variational techniques.

There are five approaches considered:

\begin{itemize}
\item Gibbs MCMC
\item Mean field Variational Bayes
\item Stochastic Variational Bayes
\item Stochastic Variational Bayes with Control Variates
\item Re-parameterised Stochastic Variational Bayes
\end{itemize}

Simulation proceeds with $T = 100$, $\phi_1 = 0.7$, $\phi_2 = 0.15$ and $\sigma^2 = 2$. Each approach estimates the same approximating distribution, $q(\theta) = \mathcal{BVN}((\phi_1, \phi_2)' | \mu, \Sigma) \mathcal{IG}(\sigma^2 | \alpha, \beta)$ with estimated parameters:

\begin{itemize}
\item $\mu_1$: The mean of $\phi_1$,
\item $\mu_2$: The mean of $\phi_2$,
\item $L_{11}, L_{21}, L_{22}$, the three components of $L$, the lower triangular decomposition of $\Sigma$,
\item $\alpha, \beta$, the shape and rate parameters of an Inverse Gamma distribution for $\sigma^2$.
\end{itemize}

The diagonal of $L$ is not forced to be positive so the decomposition is not invariant to sign changes.

Collectively, we have $\lambda = (\mu_1, \mu_2, L_{11}, L_{21}, L_{22}, \alpha, \beta)'$.

The reparameterised SVB algorithm drew $(\epsilon_1, \epsilon_2)' \sim \mathcal{N}(0, I)$ and $\epsilon_3 \sim \mathcal{U}(0, 1)$, then made the transformations:

\begin{align}
(\theta_1, \theta_2)' &= (\mu_1, \mu_2)' + L (\epsilon_1, \epsilon_2)' \nonumber \\
\theta_3 &= Q^{-1}_{IG}(\epsilon_3| \alpha, \beta) \nonumber
\end{align}

where $Q^{-1}_{IG}$ is the inverse CDF of an Inverse Gamma distribution with shape $\alpha$ and scale $\beta$.

We compare the converged values of each algorithm to the Methods of Moments (MM) estimator of the approximating distribution using the MCMC draws as data.
\vspace{3mm}
\begin{table}[hb]
\centering
<<summary, echo = FALSE>>=
library(knitr)
library(mvtnorm)
library(reshape)
summary = matrix( c(0.78, 0.16, 0.10, -0.09, 0.05, 48.9, 77.3, -76.7, 2.6,
                    0.83, 0.10, 0.04, 0, 0.05, 50, 79.7, -77.7, "1.3 x 10^{-6}", 
                    0.63, 0.27, 0.09, -0.12, 0.05, 45.6, 81.4, -78.3, 51.1, 
                    0.78, 0.15, 0.1, -0.09, 0.05, 49.3, 78, -76.7, 4.0,
                    0.77, 0.15, 0.09, -0.09, 0.05, 48.5, 77.6, -76.9, 1.3) , byrow = TRUE, ncol = 9)
                    
colnames(summary) = c(paste0(expression(mu), 1), paste0(expression(mu), 2), "L11", "L21", "L22", expression(alpha), expression(beta), "L", "time (seconds)*")
rownames(summary) = c("MCMC + MM", "Mean Field", "SVB", "SVB CV", "SVB RP")
kable(summary)
@
\caption{$\lambda$ estimates, maximised $\mathcal{L}(q(\theta)|\lambda), y)$ and runtime for each algorithm. The Mean Field algorithm is the fastest, but has not captured the posterior variance, while the original Stochastic algorithm has converged to a different local maximum than the others. MCMC with the Method of Moments and both reduced variance VB algorithms have captured almost the same distribution, with re-parameterised Stochastic VB as the fastest. All algorithms are run unparallelised, so the VB algorithms have potential for further speed increases.}
\end{table}

sga <- rmvnorm(4000, mean = c(0.77, 0.15), sigma = (L %*% t(L)))
sga <- cbind(sga, 1/rgamma(4000, shape = 48.5, rate = 77.6))
\begin{figure}[h]
<<MCMC, echo= FALSE, fig.width = 7, fig.height = 6>>=
setwd("/home/nltom2/Desktop/Confirmation")
library(ggplot2)
library(GGally)
thin <- read.csv("AR2thin.csv")[,-1]
colnames(thin) <- c("Phi_1", "Phi_2", "Sigma_Squared")
ggpairs(thin) + theme_bw()
@
\caption{Result of MCMC with 120,000 draws. The first 20\% were discarded then the remaining draws were thinned by a factor of 20, which was required to remove any dependence. The remaining effective sample size for each parameter is 5000.}
\end{figure}


\end{document}