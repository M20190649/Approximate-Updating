\documentclass{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\usepackage{natbib}
\usepackage{color}
\usepackage[dvipsnames,svgnames*]{xcolor}
\usepackage{array}
\usepackage[hidelinks]{hyperref}
\usepackage[font=small,skip=5pt]{caption}
\usepackage[aboveskip=2pt]{subcaption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage[]{algorithm2e}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{url}
\usepackage{wasysym}
\usepackage{ulem}
\usepackage{afterpage}
\usepackage{bbm}
\numberwithin{equation}{section}
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\tableofcontents
\section{Introduction} 

For many time-series applications a point forecast of the mean of the next observation in a series, $y_{T+1}$, can be easily obtained with frequentist methods, conditioning on the forecaster's observed series, $y_{1:T}$. Often the forecast is supplemented with a prediction interval backed by asymptotic theory, however there is a growing demand in the literature for density forecasts which are typically much harder to obtain, see \citet{Gneiting2014} for a review.

As a contrast to frequentism, the Bayesian methodology has been used extensively in forecasting \citep{Geweke2006} and is of interest as the entire probability density for $p(y_{T+1} | y_{1:T})$ is provided implicitly. However the Bayesian approach requires solving integrals of the form 

\begin{equation}
\label{forecastintegral}
\int_\theta p(y_{T+1} | \theta) p(\theta | y_{1:T}) d \theta,
\end{equation}

where $p(\theta | y_{1:T})$ is known as the posterior distribution, with

\begin{equation}
\label{posterior}
 p(\theta | y_{1:T}) = \frac{p(y_{1:T}|\theta)p(\theta)}{\int_\theta p(y_{1:T}|\theta)p(\theta) d\theta}
\end{equation}

and $p(\theta)$ is some pre-specified prior distribution.

These often cannot be solved analytically, while numeric integration is computationally infeasible when the dimension of $\theta$ is large. To address this problem there is a wide range of techniques used to approximate the solution to the integral in (\ref{forecastintegral}), such as Markov Chain Monte Carlo (MCMC) and Variational Bayes (VB). These approximations have an implicit trade-off: The better approximations are compuationaly intensive, and the forecaster must decide how much computation time and approximation error is acceptable. The focus of this research is in situtations where MCMC is feasible in a moderate amount of time, but is infeasible in time period between two observations. To move $p(y_{T+1} | y_{1:T})$ to $p(y_{T+2} | y_{1:T+1})$ requires updating the posterior distribution after observation of each new data point observed via

\begin{equation}
\label{posteriorupdate}
p(\theta | y_{1:T+1}) = \frac{p(y_{T+1} | \theta) p(\theta | y_{1:T})}{\int_\theta p(y_{T+1} | \theta) p(\theta | y_{1:T}) d\theta}.
\end{equation}

However, if the data is based on a market such as with stock returns the market will close overnight and MCMC can be ran.

\section{Bayesian Inference}
\subsection{Exact Bayesian Computation}
While it is technically an approximation method, MCMC algorithms result in what is often called an exact computation of the posterior. A Gibbs based MCMC iteratively samples from the conditional distributions

\begin{align}
&p(\theta_1 | \theta_2, \dots, \theta_p, y_{1:T}) \nonumber \\
&p(\theta_2 | \theta_1, \theta_3, \dots, \theta_p, y_{1:T}) \nonumber \\
&\vdots \nonumber \\
&p(\theta_p | \theta_1, \dots, \theta_{p-1}, y_{1:T}) \nonumber
\end{align}

where $p$ is the dimension of $\theta$. With enough iterations, the error in the approximation converges to zero and the algorithm can be ran for as much time as the forecaster desires to reduce error to a desired level. However, in the first iteration we must set arbitary starting values for each of $\theta_2, \dots, \theta_p$ introducing a large amount of error in the early iterations. To avoid this error MCMC generally must be run for a large number of iterations and these early samples are discarded. The computation time per iteration and speed of convergence is extremely problem specific. 

We illustrate this with an AR(2), a simple time series model used in the remainder of this section. The AR(2) is described by

\begin{equation}
\label{AR2}
y_t - \mu = \rho_1 (y_{t-1} - \mu) + \rho_2 (y_{t-2} - \mu) + \epsilon_t
\end{equation}

where $\epsilon_t \sim \mathcal{N}(0, \sigma^2)$. We collect the unknown parameters as $\theta = (\mu, \sigma^2, \rho_1, \rho_2)'$ and set priors as

\begin{align}
p(\mu) &\propto 1 \nonumber \\
p(\sigma^2) &\propto \sigma^{-2} \nonumber \\
\mbox{Some kind of }&\rho \mbox{ prior}
\end{align}

as this prior on $\rho_1$ and $\rho_2$ is uniform over the triangle that defines the AR(2) stationary region.

Describe conditional distributions, possible MH step, result in forecast distribution for $y_{T+1}$

\subsection{Variational Bayes}

Variational Bayes introduces some approximating distribution $q(\theta | \lambda)$ and aims to choose the family $q$ and set of parameters $\lambda$ so that $q(\theta | \lambda$ is as close as possible to the true posterior $p(\theta | y)$. It does this by minimising the Kullback-Leibler (KL) divergence \citep{Kullback1951} from $q(\theta | \lambda)$ to $p(\theta | y)$. The KL divergence is defined by

\begin{equation}
\label{KL-def}
KL[q(\theta | \lambda)||p(\theta | y)] = \int q(\theta | \lambda) \ln \left( \frac{q(\theta | \lambda)}{p(\theta | y)}\right) d\theta.
\end{equation}

and is a non-negative, assymetric measure of the difference between $p(\theta | y)$ and $q(\theta | \lambda)$ that will equal zero if and only if $p(\theta | y) = q(\theta | \lambda)$ almost everywhere \citep{Bishop2006}. It has origins in information theory, and can be interpreted as: \textit{`Given that I know $p(\theta | y)$ for some $\theta \in \Theta$, how much extra information is required, on average, to know the value of $q(\theta | \lambda)$?'} There are examples of the literature of approximations with other measures of divergence, such as \citet{Minka2001} introducting Expectation Propogation (EP), which minimises the reverse measure $KL[p(\theta | y)||q(\theta |\lambda)]$, which was extended to Power-EP in \citet{Minka2004}, which aims to minimise the more general $\alpha-\mbox{divergence}$ \citep{Amari1985}. It is shown in \citet{Bishop2006} (note: find original proof) that minimising $KL[p(\theta | y)||q(\theta | \lambda)]$ used in in EP is equivalent to the MLE of $q(\theta | \lambda)$ given a sample of $\theta$. However, we can write $KL[q(\theta | \lambda)||p(\theta | y)]$ as

\begin{equation}
\label{KL-ELBO}
KL[q(\theta | \lambda)||p(\theta | y)] = \ln(y) - \mathcal{L}(q, y)
\end{equation}

where $\mathcal{L}(q, y)$ is known as the Evidence Lower BOund (ELBO), defined by

\begin{equation}
\label{ELBO}
\mathcal{L}(q, y) = \int_{\theta} q(\theta|\lambda) \ln (p(y, \theta|\lambda)) d\theta -  \int_{\theta} q(\theta|\lambda) \ln (q(\theta|\lambda)) d\theta.
\end{equation}

From (\ref{ELBO}) it is clear that maximising $\mathcal{L}(q, y)$ with respect to $q$ is equivalent to minimising (\ref{KL-def}). Maximising the ELBO is much more convenient than minimising either form of the KL Divergence, and has lead to Variational Bayes been used much more widely in the literature than alternatives such as EP.

\subsubsection{Mean Field Variational Bayes} 

Mean Field Variational Bayes (MFVB) has origins in the physics literature \citep{Chandler1987} and restricts the search for an optimal approximating dististribution to the set of factorisable distributions

\begin{equation}
\label{mf1}
q(\theta|\lambda) = \prod_i q_i(\theta_i | \lambda_i).
\end{equation}

This technique is widely used as it greatly simplifies maximisation of the ELBO (\citealp{Jordan1999}; \citealp{Bishop2006}), particularly in exponential family models \citep{Wainwright2008}. Each component $\theta_i$ may be a scalar or a vector, and $\lambda_i$ is the component of the $\lambda$ vector that parameterises the relevant factor $q_i(\theta_i |\lambda_i)$. From here, we will use the shorthand notation that $q_i = q_i(\theta_i|\lambda_i)$ and $q_{\setminus i} = \prod_{j\neq i}q_j$. Maximising the ELBO with respect to $q_i$ is analytically involved but computationally simple, as (\ref{ELBO}) can be expressed as a function of $q_i$ only and then maximised with respect to each $q_i$ individually. \citet{Attias1999} shows that 

\begin{equation}
\label{mf2}
q(\theta_i | \lambda_i) \propto\exp( \mathbb{E}_{q_{\setminus i}} [\ln(p(y,\theta))])
\end{equation}

and maximisation can proceed by matching (\ref{mf2}) to a known distribution. In the case that the likelihood and prior for $\theta_i$ form an exponential family conjugate pair, then the distributional family can be kept constant and all that is required is an update the parameters $\lambda_i$. If it does not match a known distribution, we may need to make a further approximation $\tilde{q_i(\theta_i|\lambda_i)}$ which has a recognizable distribution. One method as used in \citet{Friston2006} uses a Laplace approximation to and substitute in a Gaussian distribution for an otherwise unrecognizable $q_i(\theta_i | \lambda_i)$. 

Matching distributions in this way is a very similar method to finding the posterior conditional distribution, $p(\theta_i | y, \theta_{\setminus i}$ in a Gibbs MCMC scheme, with dependence on other parameters replaced by their expectations. Hence, the optimal approximating family for $\theta_i$, $q(\theta_i | \lambda_i)$, will come from the same distributional family as the conditional distribution $p(\theta_i | \theta_{j \neq i}, y)$, if it exists in a recognisable form such as the exponential family. The secondary Laplace approximation is analogous to a Metropolis-Hastings-within-Gibbs step in MCMC, as a way to handle unrecognisable distributions.
\vspace{5mm}

Given these optimal distributional families, a mean field updating equation for each $\lambda_i$ can be found as a function of the data and other $\lambda_{j \neq i}$. This dependence requires an algorithm that continuously iterates between each $\lambda_i$ and sets it to its maximising value until $\mathcal{L}(q(\theta | \lambda), y)$ converges within some pre-defined threshold. This is known as a coordinate ascent algorithm and follows below, where $k$ as the dimension of $\lambda$.

\vspace{2mm}

\begin{algorithm}[H]
 \SetKwInOut{Input}{Input}
 \Input{Log Joint Density}
 \KwResult{Mean Field Approximation}
 Initialise $\lambda$ randomly\;
 Use (\ref{mf3}) to match each $q(\theta_i|\lambda_i)$ to a tractable distribution\;
 \While{Not converged}{
  \For{$i =  1$ \KwTo $k$}{
      Hold $\lambda_{j \neq i}$ fixed\;
      Update $\lambda_i$ using $y$ and the most recent values of $\lambda_{j \neq i}$\;
     }
 }
 \caption{Coordinate Ascent for MFVB}
  \label{alg:algorithm1}
\end{algorithm}

\subsubsection{Stochastic Variational Bayes}

In many cases, the posterior is too complex to be captured for a factorising approximation to be reasonabl as we desire our distribution $q(\theta | \lambda)$ to capture dependence between parameters. In this case, the easy maximisation in MFVB is unavailable, and we must resort to what is called Stochastic Variational Bayes (SVB) using a gradient ascent algorithm developed by \citet{Paisley2012} and \citet{Ranganath2014}.

To maximise the function $\mathcal{L}(q(\theta | \lambda), y)$ we can take the derivative of $\mathcal{L}(q(\theta | \lambda), y)$ with respect to $\lambda$ and use the updating step:

\begin{equation}
\label{SGA1}
\lambda^{(m+1)} = \lambda^{(m)} + \rho^{(m)} \nabla_{\lambda} \mathcal{L}(q(\theta | \lambda^{(m)}), y),
\end{equation}

where $\nabla_{\lambda}\mathcal{L}(q(\theta | \lambda^{(m)}), y)$ is the vector of partial derivatives of $\mathcal{L}(q(\theta | \lambda^{(m)}), y)$ with respect to each element of $\lambda$. This update requires initial values $\lambda^{(0)}$ and some learning rate sequence $\rho^{(m)}$. If $\rho^{(m)}$ is chosen to satisfy the following conditions, it is a Robbins-Monro sequence and the algorithm is guaranteed to converge to a local maximum.

\begin{align}
\sum_{m=1}^{\infty} \rho^{(m)} &=  \infty \\
\sum_{m=1}^{\infty} (\rho^{(m)})^2 &<  \infty.
\end{align}

Whilst a global maximum is desired, the ELBO can have a problem specific shape that makes finding the global maximum extremely difficult, as we do not know how many stationary points exist. The dimension of the ELBO is the dimension of the $\lambda$ vector, which is often much greater than the dimension of the parameters $\theta$ so a grid search is suspect to the curse of dimensionality. To alleiviate this problem, we can start the algorithm at a range of initial values choose the converged value with the highest value of the ELBO. 

SVB does not provide the family of the the optimal approximating distribution, unlike under the mean field assumption. To run SVB we may need to try many approximating distributions and then choose the $q(\theta | \lambda)$ that has the highest ELBO, and hence lowest KL divergence to the true posterior. We must restrict the approximation to distributions that satisfy the condition that the order of differentation of the ELBO with respect to $\lambda$ and integration with respect to $\theta$ are interchangable. In this case \citet{Ranganath2014} shows that a Monte Carlo estimate of the derivative of the ELBO can be taken by

\begin{equation}
\label{SGA2}
\nabla_{\lambda}\mathcal{L}(q(\theta | \lambda^{(m)}) \approx \frac{1}{S}\sum_{s=1}^{S} \nabla_{\lambda} [\ln(q(\theta_s | \lambda^{(m)}))] (\ln (p(y, \theta_s)) - \ln(q(\theta_s | \lambda^{(m)})))
\end{equation}

where $s = 1, \dots, S$ indicates simulations from $q(\theta | \lambda^{(m)})$.

As the distribution $q(\theta | \lambda)$ is specified by the user, the main restriction on the use of SVB is that the log-joint density $\ln(p(y, \theta))$ is able to be evaluated.

\citet{Duchi2011} introduces the AdaGrad algorithm which can be implemented within SVB to control $\rho^{(m)}$. AdaGrad allows each $\lambda_i$ to have an independent $\rho^{(m)}_i$ that is inversely proportional to the gradient. 

Let 

\begin{equation}
\label{SGA3}
G_i^{(m)} = \sum_{j = 1}^{m} \left(\nabla_{\lambda_i}\mathcal{L}(q(\theta | \lambda^{(m)})\right)^2,
\end{equation}

then each component's learning rate is defined as

\begin{equation}
\label{SGA4}
\rho^{(m)}_i = \eta \left(G_i^{(m)}\right)^{-1/2}
\end{equation}

for some tuning parameter $\eta$.

The resulting Stochastic Gradient Ascent algorithm proceeds below.

\begin{algorithm}[H]
 \SetKwInOut{Input}{Input}
 \Input{Log Joint Density, Approximation family q}
 \KwResult{Variational Approximation}
 Initialise $\lambda$\;
 \While{Not converged}{
  Simulate $\theta^s$ for $s = 1, \dots S$ from $q(\theta|\lambda^{(m)})$\;
  \For{$i =  1$ \KwTo $k$}{
      Calculate $\nabla_{\lambda_i} =  1/S\sum_{s=1}^{S} \nabla_{\lambda_i}[\log(q(\theta^s|\lambda^{(m)})](\log(p(\theta^s, y)) - \log(q(\theta^s |\lambda^{(m)})))$\;
      Update $G_i^{(m)} = G_i^{(m-1)} + \left(\nabla_{\lambda_i}\mathcal{L}(q(\theta | \lambda^{(m)})\right)^2$\;
      Calculate $\rho^{(m)}_i =  \left(G_i^{(m)}\right)^{-1/2}$\;
     }
  Set $\lambda^{(m+1)} = \lambda^{(m)} + \rho^{(m)}  \nabla_{\lambda}$\;
  Set $m = m + 1$\;
 }
 \caption{Stochastic Gradient Ascent for SVB}
  \label{alg:algorithm2}
\end{algorithm}

\subsection{Variance Reduction Techniques}

For many models the estimator in (\ref{SGA2}) has a large variance, so a more efficient estimator will allow a lower value of $S$ and hence reduced computation time.
\subsubsection{Control Variates}
\citet{Paisley2012} considers the use of control variates to augment the estimator of the gradient. Define

\begin{equation}
\label{CV1}
g(\theta, \lambda, y) =  \nabla_{\lambda} [\ln(q(\theta_s | \lambda))] (\ln (p(y, \theta_s)) - \ln(q(\theta_s | \lambda))),
\end{equation}

the function estimated in (\ref{SGA2}), then for some other function $h$, define

\begin{equation}
\label{CV2}
\hat{g}(\theta, \lambda, y) = g(\theta, \lambda, y) - a(h - \mathbb{E}(h)).
\end{equation}

Then both $g(\theta, \lambda, y)$ and $\hat{g}(\theta, \lambda, y)$ have the same expectation and

\begin{equation}
\label{CV3}
\mbox{Var}(\hat{g}) = \mbox{Var}(g) + a^2 \mbox{Var}(h) - 2a\mbox{Cov}(g, h). 
\end{equation}

Solving the polynomial (\ref{CV3}) in $a$ shows that the variance of $\hat{g}$ is minimised by 

\begin{equation}
\label{CV4}
\hat{a} = \mbox{Cov}(g, h)/\mbox{Var}(h).
\end{equation}

Substituting (\ref{CV4}) into (\ref{CV3}) yields

\begin{equation}
\label{CV5}
\mbox{Var}(\hat{g}) = \mbox{Var}(g) - \mbox{Cov}(g, h)^2/\mbox{Var}(h).
\end{equation}

We need to choose some function $h$ that has a large covariance with $g$. \citet{Ranganath2014} suggests that the easy option of

\begin{equation}
\label{CV6}
h(\theta, \lambda) = \nabla_{\lambda} [\ln(q(\theta_s | \lambda))].
\end{equation}

As $h$ is a score function, it has an expectation of zero and our estimator becomes

\begin{equation}
\label{CV7}
\nabla_{\lambda} \mathcal{L}(q(\theta | \lambda), y) \approx \frac{1}{S}\sum_{s=1}^{S} \nabla_{\lambda} [\ln(q(\theta_s | \lambda))] (\ln (p(y, \theta_s)) - \ln(q(\theta_s | \lambda)) - \hat{a}),
\end{equation}

where $\hat{a}$ is estimated from the sample variance and covariance of a subset of the $S$ Monte-Carlo draws.

\begin{algorithm}[H]
 \SetKwInOut{Input}{Input}
 \Input{Log Joint Density, Approximation family q}
 \KwResult{Variational Approximation}
 Initialise $\lambda$\;
 \While{Not converged}{
  Simulate $\theta_s$ for $s = 1, \dots S$ from $q(\theta|\lambda^{(m-1)})$\;
  \For{$i =  1$ \KwTo $k$}{
      Calculate $g_{\lambda_i} = 1/S\sum_{s=1}^{S} \nabla_{\lambda_i}[\log(q(\theta^s|\lambda^{(m-1)})](\log(p(\theta^s, y)) - \log(q(\theta^s |\lambda^{(m-1)})))$\;
      Calculate $h_{\lambda_i} = 1/S\sum_{s=1}^{S}\nabla_{\lambda_i}[\log(q(\theta^s |\lambda^{(m-1)}))]$\;
      Use a subset of $S$ to estimate $\hat{a} = \mbox{Cov}(h, g)/\mbox{Var}(g)$\;
      Calculate $\nabla_{\lambda_i} = g_{\lambda_i} - \hat{a} h_{\lambda_i}$\;
     }
  Set $\lambda^{(m)} = \lambda^{(m-1)} + \rho^{(m)}  \nabla_{\lambda}$\;
  Set $m = m + 1$\;
 }
 \caption{Gradient Ascent for SVB with control variates}
  \label{alg:algorithm3}
\end{algorithm}

\subsubsection{Reparameterisation}

The reparameterisation trick was popularised following \citet{Kingma2013} and allows the gradient of the ELBO to be simplified.

Consider a parameter free auxillary distribution $q(\epsilon)$ and differentiable transformation $f(\cdot,\cdot)$ such that $\theta = f(\epsilon, \lambda)$. Examples include a location-scale transformation from a standard normal or an inverse-CDF transform from a uniform$(0, 1)$ variable. Note that 

$$q_\theta(\theta | \lambda) = q_\epsilon(\epsilon) \left| \frac{d\epsilon}{d\theta} \right| $$

where the parameters that govern the transform $f$ are the same $\lambda$ parameters as in $q(\theta | \lambda)$. Now (\ref{SGA2}) becomes

\begin{align}
\nabla_{\lambda} \mathcal{L}(q(\theta | \lambda), y) &=  \int_{\epsilon} \nabla_{\lambda}[  q(\epsilon) \left( \ln (p(y, f(\epsilon, \lambda))) - \ln(q(f(\epsilon, \lambda) | \lambda) \right)] d\epsilon \nonumber \\
&=  \int_{\epsilon}  q(\epsilon) \nabla_{\lambda} \left[ \ln (p(y, f(\epsilon, \lambda))) - \ln(q(f(\epsilon, \lambda)| \lambda) \right] d\epsilon \label{RP1} \\
&= \int_{\epsilon}  q(\epsilon) \nabla_{\lambda} \left[ \ln (p(y, f(\epsilon, \lambda)) \right] d\epsilon\nonumber
\end{align}

as the second part on the right hand side of (\ref{RP1}) is a score function with expectation equal to zero. This can be estimated using 

\begin{equation}
\label{RP2}
 \mathcal{L}(q(\theta | \lambda), y) \approx  \frac{1}{S}\sum_{s=1}^{S} \nabla_{\lambda} \left[ \ln (p(y, f(\epsilon_s, \lambda))) \right].
\end{equation}

with $\epsilon_s \sim q(\epsilon)$.

\citet{Kingma2015} explore how this reparameterised version can have orders of magnitude lower variance than the estimator in (\ref{SGA2}).

\begin{algorithm}[H]
 \SetKwInOut{Input}{Input}
 \Input{Log Joint Density, Approximation family q}
 \KwResult{Variational Approximation}
 Initialise $\lambda$\;
 \While{Not converged}{
  Simulate $\epsilon^s$ for $s = 1, \dots S$\;
  \For{$i =  1$ \KwTo $k$}{
      Calculate $\nabla_{\lambda_i} =  1/S\sum_{s=1}^{S} \nabla_{\lambda_i}[\log(p(f(\epsilon^s, \lambda^{(m-1)}), y))]$
     }
  Set $\lambda^{(m)} = \lambda^{(m-1)} + \rho^{(m)} \nabla_{\lambda}^{t}$\;
  Set $m = m + 1$\;
 }
 \caption{Gradient Ascent for re-parameterised SVB}
  \label{alg:algorithm4}
\end{algorithm}

\subsection{AR2 model example (Revisited using VB)}

As the use of SVB does not provide an optimal approximating distribution like in MFVB, we instead analyse the MCMC results and fit a parametric distribution to the MCMC samples. \citet{Tran2015} extends SVB for use in approximations with a vine copula dependency structure, so we adopt their approach.

As fitting a distribution to a sample from the posterior by MLE is equivalent to minimising the reverse of the KL dependence, we use the best fitting set of marginal distributions augmented by a vine copula by AIC (or BIC or something else?) and make the assumption that the family $q(\theta | \lambda)$ that minimises the reverse KL, and is optimal in EP, is the same family that is optimal for VB.

\begin{itemize}
\item State optimal families
\item Fit via SVB
\item Compare forecast accuracy to MCMC 
\item Maybe use a lot of one step ahead forecasts with continuous VB updates compared to MCMC results with no parameter updates 
\end{itemize}

\section{Electricity Load Forecasts}
\subsection{Motivation}
\begin{itemize}
\item Describe and plot Data
\item Real data has five minute updates
\item Density is of interest due to rare price spikes - show price data/supply curve etc to demonstrate
\item Some general background on load forecasting somewhere here
\end{itemize}

\subsection{Exponential Smoothing}

\citet{Taylor2003} explores the seasonal properties of electricity demand and introduces a double seasonal Holt-Winters exponential smoothing model with daily and weekly effects described by (\ref{ds-hw1})-(\ref{ds-hw4}), and in \citet{Taylor2008} verifies that this approach is superior to other common time-series models for very short-term load forecasting \citep{Taylor2008}.

\begin{align}
y_t &= l_{t-1} + d_{t-m_1} + w_{t-m_2} + e_t \label{ds-hw1} \\
l_t &= \alpha (y_t - d_{t-m_1} - w_{t-m_2}) + (1 - \alpha)l_{t-1} \label{ds-hw2}\\
d_t &= \delta (y_t - l_{t-1} - w_{t-m_2}) + (1 - \delta)d_{t-m_1} \label{ds-hw3} \\
w_t &= \omega (y_t - l_{t-1} - d_{t-m_1}) + (1 - \omega)w_{t-m_2} \label{ds-hw4}
\end{align}

where $m_1$ and $m_2$ are the lengths of the daily and weekly cycle, restricting the smoothing parameters $\alpha, \delta, \omega$ to lie in $(0, 1)$. This can be rewritten as a single source of error state-space model \citep{Snyder1985},

\begin{align}
y_t &= l_{t-1} + d_{t-m_1} + w_{t-m_2} + e_t \label{ds-hw-ssoe1} \\
l_t &= l_{t-1} + \alpha e_t \label{ds-hw-ssoe2} \\
d_t &= d_{t-m_1} + \delta e_t \label{ds-hw-ssoe3} \\
w_t &= w_{t-m_2} + \omega e_t \label{ds-hw-ssoe4}. 
\end{align}

\citet{Forbes2000} describes a Bayesian method to sample the posterior distribution  when $e_t \sim \mathcal{N}(0, \sigma^2)$. This method involves inverting a matrix that is singular using the parameterisation described by (\ref{ds-hw-ssoe1}) - (\ref{ds-hw-ssoe4}) so we reparameterise the seasonality using (\ref{ds-hw-rp1}) - (\ref{ds-hw-rp4}) to avoid this problem.

\begin{align}
y_t &= l_{t-1} - \sum_{i = 1}^{m_1 - 1}d_{t-i} - \sum_{i = 1}^{m_2 - 1}w_{t-i} + e_t \label{ds-hw-rp1} \\
l_t &= l_{t-1} + \alpha e_t \label{ds-hw-rp2} \\
d_t &= - \sum_{i = 1}^{m_1 - 1}d_{t-i} + \delta e_t \label{ds-hw-rp3} \\
w_t &= - \sum_{i = 1}^{m_2 - 1}w_{t-i} + \omega e_t \label{ds-hw-rp4}.
\end{align}

The resulting unknown parameters are $\theta = (\alpha, \delta, \omega)', \sigma^2$ and $b_0$, the $m_1 + m_2 - 1$ length vector of the initial states of $l_0, d_0, \dots, d_{-(m_1 - 2)}, w_0, \dots, w_{-(m_2 - 2)}$.
\citet{Forbes2000} recommends numerical integration of the marginal posterior density of $\theta$,

\begin{equation}
\label{exp-sm-marginal}
p(\theta | y_{1:T}) \propto \left| \widetilde{X}' \widetilde{X} \right|^{-1/2} \tilde{s}^{-(T-(m_1 + m_2 - 1))} p(\theta),
\end{equation}

where $\widetilde{X}$ and $\tilde{s}$ are functions of $\theta$ and $y$. The problems with using this method in our model are two-fold: repeating numerical integrion each time a new data point is observed for a three dimensional distribution is computationally difficult, and the $T$ in the exponent makes most evalutions of $p(\theta | y_{1:T})$ computationally zero when $T$ is large.

\begin{itemize}
\item Derive some MCMC algorithm using $\theta$ conditional
\item Infer model structure from draws if we can run this
\item Or pick as flexible as possible distribution for marginals (normal mixture?) and figure out some vine structure to run VB without MCMC
\end{itemize}

\subsection{Variational Bayes Implementation}
\begin{itemize}
\item Posterior Distributions hopefully do not change much as more data is observed, so we can keep approximating distribution family constant
\item But we might want a model that does change to make updates worthwhile
\item Ideal model has constant distributional family with changing parameter values
\item Maybe via Markov Switching mechanism to be added as probability of being in a certain state should change.
\end{itemize}
\subsection{Forecasting}

\begin{itemize}
\item Compare to MCMC when possible
\end{itemize}

\section{Timeline}
\begin{itemize}
\item Put something here eventually
\end{itemize}

\bibliographystyle{asa}
\bibliography{references}

\end{document}
\grid
\grid
