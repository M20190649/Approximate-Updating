\documentclass{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)

%2multibyte Version: 5.50.0.2960 CodePage: 1252
\documentclass[12pt,a4paper]{article}%
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[centertags]{amsmath}
\usepackage{graphicx}%
\usepackage{natbib}
\usepackage{color}
\usepackage[dvipsnames,svgnames*]{xcolor}
\usepackage{array}
\usepackage[hidelinks]{hyperref}
\usepackage[font=small,skip=5pt]{caption}
\usepackage[aboveskip=2pt]{subcaption}
\usepackage{amsmath}
\usepackage[]{algorithm2e}
\usepackage{amsthm}
\usepackage{url}
\usepackage{wasysym}
\usepackage{ulem}
\usepackage{afterpage}
\usepackage{bbm}
\setcounter{MaxMatrixCols}{30}
\providecommand{\U}[1]{\protect\rule{.1in}{.1in}}
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\textbf{#1.} }{\  \rule{0.5em}{0.5em}}
\setlength{\topmargin}{0in}
\setlength{\oddsidemargin}{0.1in}
\setlength{\evensidemargin}{0.1in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8.25in}
\title{Real Time Variational Density Forecasts}
\author{Nathaniel Tomasetti, Catherine Forbes and Anastasios Panagiotelis}
\begin{document}

\maketitle
\tableofcontents
\section{Introduction} 

For many time-series applications a point forecast of the mean of the next observation in a series, $y_{T+1}$, can be easily obtained with frequentist methods, conditioning on the forecaster's observed series, $y_{1:T}$. Often the forecast is supplemented with a prediction interval backed by asymptotic theory, however there is a growing demand in the literature for density forecasts which are typically much harder to obtain, see \citet{Gneiting2014} for a review.

As a contrast to frequentism, the Bayesian methodology has been used extensively in forecasting \citep{Geweke2006} and is it can capture the error components from parameter estimation and modelling error. However calculation of the Bayesian posterior distribution, $p(\theta | y)$ requires solving complex integrals that often do not have an analytical solution, in addition, if the the dimension of the parameter vector $\theta$ is large then numerical integration will be computationally infeasible. 

To address this problem a wide range of techniques are available, such as Markov Chain Monte Carlo (MCMC) and Variational Bayes (VB). The forecasters decision of technique to use is a compromise to implicit trade-off: MCMC captures the posterior distribution accurately but is compuationaly intensive, while other methods such as VB tend to be faster but are inaccurate. One time series of interest, electricity demand data, operates on five minute interval in Victoria, too short for MCMC to be a feasible option in a moderately complex model. The focus of this research is in time series that operate in real-time, where the observation of each data point requires the model parameters to be updated before a density forecast of the next data point can be made. Variational Bayes is used as an alternative computation strategy, and the trade off between the statistical and computational efficiency will be explored using data on the electricity demand of Victoria for the 2012-2015 period.

\section{Bayesian Inference}

Exact Bayesian inference in the context of forecasting uses the predictive density of $Y_{T+1}$ conditioned on the observed series, $Y_{t}, t = 1, \dots T$, which is denoted by $y_{1:T}$. The first step requires the posterior distribution of the model parameters $\theta$, p(\theta | y), where

\begin{equation}
\label{posterior}
 p(\theta | y_{1:T}) = \frac{p(y_{1:T}|\theta)p(\theta)}{\int_\theta p(y_{1:T}|\theta)p(\theta) d\theta}.
\end{equation}

Generally the solution to (\ref{posterior}) is unavailable. This section will offer two alternatives, MCMC and VB.
MCMC is used to create a sample from $p(\theta | y)$, and any function of interest from $p(\theta | y$ can be found with the relevant function applied to the sample, while VB replaces $p(\theta | y)$ with an approximation $q(\theta | \lambda)$, choosing some family $q$ and approximation parameters $\lambda$.

\subsection{Markov Chain Monte Carlo}

There are many types of MCMC algorithm, the most common being a Gibbs sampler, which iteratively samples from a $p$ dimensional $\theta$ via the conditional distributions

\begin{align}
&p(\theta_1 | \theta_2, \dots, \theta_p, y_{1:T}) \nonumber \\
&p(\theta_2 | \theta_1, \theta_3, \dots, \theta_p, y_{1:T}) \nonumber \\
&\vdots \nonumber \\
&p(\theta_p | \theta_1, \dots, \theta_{p-1}, y_{1:T}). \nonumber
\end{align}

With enough iterations, these samples converge in distribution to $p(\theta | y)$ and can be used for inference. The computation time per iteration and number of iterations required for convergence is problem specific but typically increases with model complexity. 

To illustrate issues involved consider a simple auto-regressive time series model which will be used in the remainder of this section. This model is described by

\begin{equation}
\label{AR2}
y_t = \rho_1 y_{t-1} + \rho_2 y_{t-2} + \epsilon_t
\end{equation}

where $\epsilon_t \sim \mathcal{N}(0, \sigma^2)$. The likelihood of parameters $\theta = (\rho_1, \rho_2m \sigma^2)'$ is given by

\begin{align}
\label{likelihood}
L(\theta | y_{1:T}) &= p(y_1 | \theta) p(y_2 | y_1, \theta) \prod_{t=3}^{T}p(y_t | y_{1:t-1}, \theta) \nonumber \\
&= \frac{1}{(2\pi)^{T/2}\gamma(0)^{1/2}\Gamma{0}^{1/2}\sigma^{(T-2)/2}} \exp \left\{ \frac{-y_1^2}{2 \gamma(0)} \right\} 
\exp \left\{\frac{-(y_2 - \gamma(1)/\gamma(2)y_1)^2}{2 \Gamma(0)} \right} \exp \left\{ \frac{-\sum_{t=3}^{T}(y_t - \phi_1 y_{t-1} - \phi_2 y_{t-2})^2}{2\sigma^2} \right\}
\end{align}

where $\gamma(k)$ is the $k-th$ autocorrelation and $\Gamma(1) = \gamma(0) - \gamma(1)^2 / \gamma(0)$.

\begin{align}
p(\mu) &\propto 1 \nonumber \\
p(\sigma^2) &\propto \sigma^{-2} \nonumber \\
\mbox{Some kind of }&\rho \mbox{ prior}
\end{align}

as this prior on $\rho_1$ and $\rho_2$ is uniform over the triangle that defines the AR(2) stationary region.

Describe conditional distributions, possible MH step, result in forecast distribution for $y_{T+1}$

\subsection{Variational Bayes}

Variational Bayes introduces some approximating distribution $q(\theta | \lambda)$ and aims to choose the family $q$ and set of parameters $\lambda$ so that $q(\theta | \lambda$ is as close as possible to the true posterior $p(\theta | y)$. It does this by minimising the Kullback-Leibler (KL) divergence \citep{Kullback1951} from $q(\theta | \lambda)$ to $p(\theta | y)$. The KL divergence is defined by

\begin{equation}
\label{KL-def}
KL[q(\theta | \lambda)||p(\theta | y)] = \int q(\theta | \lambda) \ln \left( \frac{q(\theta | \lambda)}{p(\theta | y)}\right) d\theta.
\end{equation}

and is a non-negative, assymetric measure of the difference between $p(\theta | y)$ and $q(\theta | \lambda)$ that will equal zero if and only if $p(\theta | y) = q(\theta | \lambda)$ almost everywhere \citep{Bishop2006}. It has origins in information theory, and can be interpreted as: \textit{`Given that I know $p(\theta | y)$ for some $\theta \in \Theta$, how much extra information is required, on average, to know the value of $q(\theta | \lambda)$?'} There are examples of the literature of approximations with other measures of divergence, such as \citet{Minka2001} introducting Expectation Propogation (EP), which minimises the reverse measure $KL[p(\theta | y)||q(\theta |\lambda)]$, which was extended to Power-EP in \citet{Minka2004}, which aims to minimise the more general $\alpha-\mbox{divergence}$ \citep{Amari1985}. It is shown in \citet{Bishop2006} (note: find original proof) that minimising $KL[p(\theta | y)||q(\theta | \lambda)]$ used in in EP is equivalent to the MLE of $q(\theta | \lambda)$ given a sample of $\theta$. However, we can write $KL[q(\theta | \lambda)||p(\theta | y)]$ as

\begin{equation}
\label{KL-ELBO}
KL[q(\theta | \lambda)||p(\theta | y)] = \ln(y) - \mathcal{L}(q, y)
\end{equation}

where $\mathcal{L}(q, y)$ is known as the Evidence Lower BOund (ELBO), defined by

\begin{equation}
\label{ELBO}
\mathcal{L}(q, y) = \int_{\theta} q(\theta|\lambda) \ln (p(y, \theta|\lambda)) d\theta -  \int_{\theta} q(\theta|\lambda) \ln (q(\theta|\lambda)) d\theta.
\end{equation}

From (\ref{ELBO}) it is clear that maximising $\mathcal{L}(q, y)$ with respect to $q$ is equivalent to minimising (\ref{KL-def}). Maximising the ELBO is much more convenient than minimising either form of the KL Divergence, and has lead to Variational Bayes been used much more widely in the literature than alternatives such as EP.

\subsubsection{Mean Field Variational Bayes} 

Mean Field Variational Bayes (MFVB) has origins in the physics literature \citep{Chandler1987} and restricts the search for an optimal approximating dististribution to the set of factorisable distributions

\begin{equation}
\label{mf1}
q(\theta|\lambda) = \prod_i q_i(\theta_i | \lambda_i).
\end{equation}

This technique is widely used as it greatly simplifies maximisation of the ELBO (\citealp{Jordan1999}; \citealp{Bishop2006}), particularly in exponential family models \citep{Wainwright2008}. Each component $\theta_i$ may be a scalar or a vector, and $\lambda_i$ is the component of the $\lambda$ vector that parameterises the relevant factor $q_i(\theta_i |\lambda_i)$. From here, we will use the shorthand notation that $q_i = q_i(\theta_i|\lambda_i)$ and $q_{\setminus i} = \prod_{j\neq i}q_j$. Maximising the ELBO with respect to $q_i$ is analytically involved but computationally simple, as (\ref{ELBO}) can be expressed as a function of $q_i$ only and then maximised with respect to each $q_i$ individually. \citet{Attias1999} shows that 

\begin{equation}
\label{mf2}
q(\theta_i | \lambda_i) \propto\exp( \mathbb{E}_{q_{\setminus i}} [\ln(p(y,\theta))])
\end{equation}

and maximisation can proceed by matching (\ref{mf2}) to a known distribution. In the case that the likelihood and prior for $\theta_i$ form an exponential family conjugate pair, then the distributional family can be kept constant and all that is required is an update the parameters $\lambda_i$. If it does not match a known distribution, we may need to make a further approximation $\tilde{q_i(\theta_i|\lambda_i)}$ which has a recognizable distribution. One method as used in \citet{Friston2006} uses a Laplace approximation to and substitute in a Gaussian distribution for an otherwise unrecognizable $q_i(\theta_i | \lambda_i)$. 

Matching distributions in this way is a very similar method to finding the posterior conditional distribution, $p(\theta_i | y, \theta_{\setminus i}$ in a Gibbs MCMC scheme, with dependence on other parameters replaced by their expectations. Hence, the optimal approximating family for $\theta_i$, $q(\theta_i | \lambda_i)$, will come from the same distributional family as the conditional distribution $p(\theta_i | \theta_{j \neq i}, y)$, if it exists in a recognisable form such as the exponential family. The secondary Laplace approximation is analogous to a Metropolis-Hastings-within-Gibbs step in MCMC, as a way to handle unrecognisable distributions.
\vspace{5mm}

Given these optimal distributional families, a mean field updating equation for each $\lambda_i$ can be found as a function of the data and other $\lambda_{j \neq i}$. This dependence requires an algorithm that continuously iterates between each $\lambda_i$ and sets it to its maximising value until $\mathcal{L}(q(\theta | \lambda), y)$ converges within some pre-defined threshold. This is known as a coordinate ascent algorithm and follows below, where $k$ as the dimension of $\lambda$.

\vspace{2mm}

\begin{algorithm}[H]
 \SetKwInOut{Input}{Input}
 \Input{Log Joint Density}
 \KwResult{Mean Field Approximation}
 Initialise $\lambda$ randomly\;
 Use (\ref{mf3}) to match each $q(\theta_i|\lambda_i)$ to a tractable distribution\;
 \While{Not converged}{
  \For{$i =  1$ \KwTo $k$}{
      Hold $\lambda_{j \neq i}$ fixed\;
      Update $\lambda_i$ using $y$ and the most recent values of $\lambda_{j \neq i}$\;
     }
 }
 \caption{Coordinate Ascent for MFVB}
  \label{alg:algorithm1}
\end{algorithm}

\subsubsection{Stochastic Variational Bayes}

In many cases, the posterior is too complex to be captured for a factorising approximation to be reasonabl as we desire our distribution $q(\theta | \lambda)$ to capture dependence between parameters. In this case, the easy maximisation in MFVB is unavailable, and we must resort to what is called Stochastic Variational Bayes (SVB) using a gradient ascent algorithm developed by \citet{Paisley2012} and \citet{Ranganath2014}.

To maximise the function $\mathcal{L}(q(\theta | \lambda), y)$ we can take the derivative of $\mathcal{L}(q(\theta | \lambda), y)$ with respect to $\lambda$ and use the updating step:

\begin{equation}
\label{SGA1}
\lambda^{(m+1)} = \lambda^{(m)} + \rho^{(m)} \nabla_{\lambda} \mathcal{L}(q(\theta | \lambda^{(m)}), y),
\end{equation}

where $\nabla_{\lambda}\mathcal{L}(q(\theta | \lambda^{(m)}), y)$ is the vector of partial derivatives of $\mathcal{L}(q(\theta | \lambda^{(m)}), y)$ with respect to each element of $\lambda$. This update requires initial values $\lambda^{(0)}$ and some learning rate sequence $\rho^{(m)}$. If $\rho^{(m)}$ is chosen to satisfy the following conditions, it is a Robbins-Monro sequence and the algorithm is guaranteed to converge to a local maximum.

\begin{align}
\sum_{m=1}^{\infty} \rho^{(m)} &=  \infty \\
\sum_{m=1}^{\infty} (\rho^{(m)})^2 &<  \infty.
\end{align}

Whilst a global maximum is desired, the ELBO can have a problem specific shape that makes finding the global maximum extremely difficult, as we do not know how many stationary points exist. The dimension of the ELBO is the dimension of the $\lambda$ vector, which is often much greater than the dimension of the parameters $\theta$ so a grid search is suspect to the curse of dimensionality. To alleiviate this problem, we can start the algorithm at a range of initial values choose the converged value with the highest value of the ELBO. 

SVB does not provide the family of the the optimal approximating distribution, unlike under the mean field assumption. To run SVB we may need to try many approximating distributions and then choose the $q(\theta | \lambda)$ that has the highest ELBO, and hence lowest KL divergence to the true posterior. We must restrict the approximation to distributions that satisfy the condition that the order of differentation of the ELBO with respect to $\lambda$ and integration with respect to $\theta$ are interchangable. In this case \citet{Ranganath2014} shows that a Monte Carlo estimate of the derivative of the ELBO can be taken by

\begin{equation}
\label{SGA2}
\nabla_{\lambda}\mathcal{L}(q(\theta | \lambda^{(m)}) \approx \frac{1}{S}\sum_{s=1}^{S} \nabla_{\lambda} [\ln(q(\theta_s | \lambda^{(m)}))] (\ln (p(y, \theta_s)) - \ln(q(\theta_s | \lambda^{(m)})))
\end{equation}

where $s = 1, \dots, S$ indicates simulations from $q(\theta | \lambda^{(m)})$.

As the distribution $q(\theta | \lambda)$ is specified by the user, the main restriction on the use of SVB is that the log-joint density $\ln(p(y, \theta))$ is able to be evaluated.

\citet{Duchi2011} introduces the AdaGrad algorithm which can be implemented within SVB to control $\rho^{(m)}$. AdaGrad allows each $\lambda_i$ to have an independent $\rho^{(m)}_i$ that is inversely proportional to the gradient. 

Let 

\begin{equation}
\label{SGA3}
G_i^{(m)} = \sum_{j = 1}^{m} \left(\nabla_{\lambda_i}\mathcal{L}(q(\theta | \lambda^{(m)})\right)^2,
\end{equation}

then each component's learning rate is defined as

\begin{equation}
\label{SGA4}
\rho^{(m)}_i = \eta \left(G_i^{(m)}\right)^{-1/2}
\end{equation}

for some tuning parameter $\eta$.

The resulting Stochastic Gradient Ascent algorithm proceeds below.

\begin{algorithm}[H]
 \SetKwInOut{Input}{Input}
 \Input{Log Joint Density, Approximation family q}
 \KwResult{Variational Approximation}
 Initialise $\lambda$\;
 \While{Not converged}{
  Simulate $\theta^s$ for $s = 1, \dots S$ from $q(\theta|\lambda^{(m)})$\;
  \For{$i =  1$ \KwTo $k$}{
      Calculate $\nabla_{\lambda_i} =  1/S\sum_{s=1}^{S} \nabla_{\lambda_i}[\log(q(\theta^s|\lambda^{(m)})](\log(p(\theta^s, y)) - \log(q(\theta^s |\lambda^{(m)})))$\;
      Update $G_i^{(m)} = G_i^{(m-1)} + \left(\nabla_{\lambda_i}\mathcal{L}(q(\theta | \lambda^{(m)})\right)^2$\;
      Calculate $\rho^{(m)}_i =  \left(G_i^{(m)}\right)^{-1/2}$\;
     }
  Set $\lambda^{(m+1)} = \lambda^{(m)} + \rho^{(m)}  \nabla_{\lambda}$\;
  Set $m = m + 1$\;
 }
 \caption{Stochastic Gradient Ascent for SVB}
  \label{alg:algorithm2}
\end{algorithm}

\subsection{Variance Reduction Techniques}

For many models the estimator in (\ref{SGA2}) has a large variance, so a more efficient estimator will allow a lower value of $S$ and hence reduced computation time.
\subsubsection{Control Variates}
\citet{Paisley2012} considers the use of control variates to augment the estimator of the gradient. Define

\begin{equation}
\label{CV1}
g(\theta, \lambda, y) =  \nabla_{\lambda} [\ln(q(\theta_s | \lambda))] (\ln (p(y, \theta_s)) - \ln(q(\theta_s | \lambda))),
\end{equation}

the function estimated in (\ref{SGA2}), then for some other function $h$, define

\begin{equation}
\label{CV2}
\hat{g}(\theta, \lambda, y) = g(\theta, \lambda, y) - a(h - \mathbb{E}(h)).
\end{equation}

Then both $g(\theta, \lambda, y)$ and $\hat{g}(\theta, \lambda, y)$ have the same expectation and

\begin{equation}
\label{CV3}
\mbox{Var}(\hat{g}) = \mbox{Var}(g) + a^2 \mbox{Var}(h) - 2a\mbox{Cov}(g, h). 
\end{equation}

Solving the polynomial (\ref{CV3}) in $a$ shows that the variance of $\hat{g}$ is minimised by 

\begin{equation}
\label{CV4}
\hat{a} = \mbox{Cov}(g, h)/\mbox{Var}(h).
\end{equation}

Substituting (\ref{CV4}) into (\ref{CV3}) yields

\begin{equation}
\label{CV5}
\mbox{Var}(\hat{g}) = \mbox{Var}(g) - \mbox{Cov}(g, h)^2/\mbox{Var}(h).
\end{equation}

We need to choose some function $h$ that has a large covariance with $g$. \citet{Ranganath2014} suggests that the easy option of

\begin{equation}
\label{CV6}
h(\theta, \lambda) = \nabla_{\lambda} [\ln(q(\theta_s | \lambda))].
\end{equation}

As $h$ is a score function, it has an expectation of zero and our estimator becomes

\begin{equation}
\label{CV7}
\nabla_{\lambda} \mathcal{L}(q(\theta | \lambda), y) \approx \frac{1}{S}\sum_{s=1}^{S} \nabla_{\lambda} [\ln(q(\theta_s | \lambda))] (\ln (p(y, \theta_s)) - \ln(q(\theta_s | \lambda)) - \hat{a}),
\end{equation}

where $\hat{a}$ is estimated from the sample variance and covariance of a subset of the $S$ Monte-Carlo draws.

\begin{algorithm}[H]
 \SetKwInOut{Input}{Input}
 \Input{Log Joint Density, Approximation family q}
 \KwResult{Variational Approximation}
 Initialise $\lambda$\;
 \While{Not converged}{
  Simulate $\theta_s$ for $s = 1, \dots S$ from $q(\theta|\lambda^{(m-1)})$\;
  \For{$i =  1$ \KwTo $k$}{
      Calculate $g_{\lambda_i} = 1/S\sum_{s=1}^{S} \nabla_{\lambda_i}[\log(q(\theta^s|\lambda^{(m-1)})](\log(p(\theta^s, y)) - \log(q(\theta^s |\lambda^{(m-1)})))$\;
      Calculate $h_{\lambda_i} = 1/S\sum_{s=1}^{S}\nabla_{\lambda_i}[\log(q(\theta^s |\lambda^{(m-1)}))]$\;
      Use a subset of $S$ to estimate $\hat{a} = \mbox{Cov}(h, g)/\mbox{Var}(g)$\;
      Calculate $\nabla_{\lambda_i} = g_{\lambda_i} - \hat{a} h_{\lambda_i}$\;
     }
  Set $\lambda^{(m)} = \lambda^{(m-1)} + \rho^{(m)}  \nabla_{\lambda}$\;
  Set $m = m + 1$\;
 }
 \caption{Gradient Ascent for SVB with control variates}
  \label{alg:algorithm3}
\end{algorithm}

\subsubsection{Reparameterisation}

The reparameterisation trick was popularised following \citet{Kingma2013} and allows the gradient of the ELBO to be simplified.

Consider a parameter free auxillary distribution $q(\epsilon)$ and differentiable transformation $f(\cdot,\cdot)$ such that $\theta = f(\epsilon, \lambda)$. Examples include a location-scale transformation from a standard normal or an inverse-CDF transform from a uniform$(0, 1)$ variable. Note that 

$$q_\theta(\theta | \lambda) = q_\epsilon(\epsilon) \left| \frac{d\epsilon}{d\theta} \right| $$

where the parameters that govern the transform $f$ are the same $\lambda$ parameters as in $q(\theta | \lambda)$. Now (\ref{SGA2}) becomes

\begin{align}
\nabla_{\lambda} \mathcal{L}(q(\theta | \lambda), y) &=  \int_{\epsilon} \nabla_{\lambda}[  q(\epsilon) \left( \ln (p(y, f(\epsilon, \lambda))) - \ln(q(f(\epsilon, \lambda) | \lambda) \right)] d\epsilon \nonumber \\
&=  \int_{\epsilon}  q(\epsilon) \nabla_{\lambda} \left[ \ln (p(y, f(\epsilon, \lambda))) - \ln(q(f(\epsilon, \lambda)| \lambda) \right] d\epsilon \label{RP1} \\
&= \int_{\epsilon}  q(\epsilon) \nabla_{\lambda} \left[ \ln (p(y, f(\epsilon, \lambda)) \right] d\epsilon\nonumber
\end{align}

as the second part on the right hand side of (\ref{RP1}) is a score function with expectation equal to zero. This can be estimated using 

\begin{equation}
\label{RP2}
 \mathcal{L}(q(\theta | \lambda), y) \approx  \frac{1}{S}\sum_{s=1}^{S} \nabla_{\lambda} \left[ \ln (p(y, f(\epsilon_s, \lambda))) \right].
\end{equation}

with $\epsilon_s \sim q(\epsilon)$.

\citet{Kingma2015} explore how this reparameterised version can have orders of magnitude lower variance than the estimator in (\ref{SGA2}).

\begin{algorithm}[H]
 \SetKwInOut{Input}{Input}
 \Input{Log Joint Density, Approximation family q}
 \KwResult{Variational Approximation}
 Initialise $\lambda$\;
 \While{Not converged}{
  Simulate $\epsilon^s$ for $s = 1, \dots S$\;
  \For{$i =  1$ \KwTo $k$}{
      Calculate $\nabla_{\lambda_i} =  1/S\sum_{s=1}^{S} \nabla_{\lambda_i}[\log(p(f(\epsilon^s, \lambda^{(m-1)}), y))]$
     }
  Set $\lambda^{(m)} = \lambda^{(m-1)} + \rho^{(m)} \nabla_{\lambda}^{t}$\;
  Set $m = m + 1$\;
 }
 \caption{Gradient Ascent for re-parameterised SVB}
  \label{alg:algorithm4}
\end{algorithm}

\subsection{AR2 model example (Revisited using VB)}

As the use of SVB does not provide an optimal approximating distribution like in MFVB, we instead analyse the MCMC results and fit a parametric distribution to the MCMC samples. \citet{Tran2015} extends SVB for use in approximations with a vine copula dependency structure, so we adopt their approach.

As fitting a distribution to a sample from the posterior by MLE is equivalent to minimising the reverse of the KL dependence, we use the best fitting set of marginal distributions augmented by a vine copula by AIC (or BIC or something else?) and make the assumption that the family $q(\theta | \lambda)$ that minimises the reverse KL, and is optimal in EP, is the same family that is optimal for VB.

\begin{itemize}
\item State optimal families
\item Fit via SVB
\item Compare forecast accuracy to MCMC 
\item Maybe use a lot of one step ahead forecasts with continuous VB updates compared to MCMC results with no parameter updates 
\end{itemize}

\section{Electricity Load Forecasts}
\subsection{Motivation}
\begin{itemize}
\item Describe and plot Data
\item Real data has five minute updates
\item Density is of interest due to rare price spikes - show price data/supply curve etc to demonstrate
\item Some general background on load forecasting somewhere here
\end{itemize}

\subsection{Exponential Smoothing}

\citet{Taylor2003} explores the seasonal properties of electricity demand and introduces a double seasonal Holt-Winters exponential smoothing model with daily and weekly effects described by (\ref{ds-hw1})-(\ref{ds-hw4}), and in \citet{Taylor2008} verifies that this approach is superior to other common time-series models for very short-term load forecasting \citep{Taylor2008}.

\begin{align}
y_t &= l_{t-1} + d_{t-m_1} + w_{t-m_2} + e_t \label{ds-hw1} \\
l_t &= \alpha (y_t - d_{t-m_1} - w_{t-m_2}) + (1 - \alpha)l_{t-1} \label{ds-hw2}\\
d_t &= \delta (y_t - l_{t-1} - w_{t-m_2}) + (1 - \delta)d_{t-m_1} \label{ds-hw3} \\
w_t &= \omega (y_t - l_{t-1} - d_{t-m_1}) + (1 - \omega)w_{t-m_2} \label{ds-hw4}
\end{align}

where $m_1$ and $m_2$ are the lengths of the daily and weekly cycle, restricting the smoothing parameters $\alpha, \delta, \omega$ to lie in $(0, 1)$. This can be rewritten as a single source of error state-space model \citep{Snyder1985},

\begin{align}
y_t &= l_{t-1} + d_{t-m_1} + w_{t-m_2} + e_t \label{ds-hw-ssoe1} \\
l_t &= l_{t-1} + \alpha e_t \label{ds-hw-ssoe2} \\
d_t &= d_{t-m_1} + \delta e_t \label{ds-hw-ssoe3} \\
w_t &= w_{t-m_2} + \omega e_t \label{ds-hw-ssoe4}. 
\end{align}

\citet{Forbes2000} describes a Bayesian method to sample the posterior distribution  when $e_t \sim \mathcal{N}(0, \sigma^2)$. This method involves inverting a matrix that is singular using the parameterisation described by (\ref{ds-hw-ssoe1}) - (\ref{ds-hw-ssoe4}) so we reparameterise the seasonality using (\ref{ds-hw-rp1}) - (\ref{ds-hw-rp4}) to avoid this problem.

\begin{align}
y_t &= l_{t-1} - \sum_{i = 1}^{m_1 - 1}d_{t-i} - \sum_{i = 1}^{m_2 - 1}w_{t-i} + e_t \label{ds-hw-rp1} \\
l_t &= l_{t-1} + \alpha e_t \label{ds-hw-rp2} \\
d_t &= - \sum_{i = 1}^{m_1 - 1}d_{t-i} + \delta e_t \label{ds-hw-rp3} \\
w_t &= - \sum_{i = 1}^{m_2 - 1}w_{t-i} + \omega e_t \label{ds-hw-rp4}.
\end{align}

The resulting unknown parameters are $\theta = (\alpha, \delta, \omega)', \sigma^2$ and $b_0$, the $m_1 + m_2 - 1$ length vector of the initial states of $l_0, d_0, \dots, d_{-(m_1 - 2)}, w_0, \dots, w_{-(m_2 - 2)}$.
\citet{Forbes2000} recommends numerical integration of the marginal posterior density of $\theta$,

\begin{equation}
\label{exp-sm-marginal}
p(\theta | y_{1:T}) \propto \left| \widetilde{X}' \widetilde{X} \right|^{-1/2} \tilde{s}^{-(T-(m_1 + m_2 - 1))} p(\theta),
\end{equation}

where $\widetilde{X}$ and $\tilde{s}$ are functions of $\theta$ and $y$. The problems with using this method in our model are two-fold: repeating numerical integrion each time a new data point is observed for a three dimensional distribution is computationally difficult, and the $T$ in the exponent makes most evalutions of $p(\theta | y_{1:T})$ computationally zero when $T$ is large.

\begin{itemize}
\item Derive some MCMC algorithm using $\theta$ conditional
\item Infer model structure from draws if we can run this
\item Or pick as flexible as possible distribution for marginals (normal mixture?) and figure out some vine structure to run VB without MCMC
\end{itemize}

\subsection{Variational Bayes Implementation}
\begin{itemize}
\item Posterior Distributions hopefully do not change much as more data is observed, so we can keep approximating distribution family constant
\item But we might want a model that does change to make updates worthwhile
\item Ideal model has constant distributional family with changing parameter values
\item Maybe via Markov Switching mechanism to be added as probability of being in a certain state should change.
\end{itemize}
\subsection{Forecasting}

\begin{itemize}
\item Compare to MCMC when possible
\end{itemize}

\section{Timeline}
\begin{itemize}
\item Put something here eventually
\end{itemize}

\bibliographystyle{asa}
\bibliography{references}

\end{document}
\grid
