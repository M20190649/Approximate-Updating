\documentclass[12pt,a4paper]{article}%
\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)

%2multibyte Version: 5.50.0.2960 CodePage: 1252
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[centertags]{amsmath}
\usepackage{graphicx}%
\usepackage{natbib}
\usepackage{color}
\usepackage[dvipsnames,svgnames*]{xcolor}
\usepackage{array}
\usepackage[hidelinks]{hyperref}
\usepackage[font=small,skip=5pt]{caption}
\usepackage[aboveskip=2pt]{subcaption}
\usepackage{amsmath}
\usepackage{amsthm}
%\usepackage{tikz}
%\usetikzlibrary{bayesnet}
\usepackage{url}
\usepackage{ulem}
\usepackage{afterpage}
\setcounter{MaxMatrixCols}{30}
\providecommand{\U}[1]{\protect\rule{.1in}{.1in}}
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\setlength{\topmargin}{0in}
\setlength{\oddsidemargin}{0.1in}
\setlength{\evensidemargin}{0.1in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8.25in}
\setlength\parindent{0pt}
\begin{document}

\section{Updating the VB Approximation}

An issue with using the Particle Filter from $T+1$ to $T+S$ is that it only gives you an estimate of $p(y_{T+1:T+S} | \theta)$, and not $p(y_{1:T+S} | \theta)$. This could be an issue as the VB algorithm requires the distribution
\begin{equation}
\label{VBPF}
\ln(p(y_{1:T+S}, \theta, x_{T+S+1})) = \ln(p(\theta)) + \ln(p(y_{1:T+S} | \theta)) + \ln(p(x_{T+S+1} | y_{1:T+S}, \theta)).
\end{equation}
as part of the ELBO,
\begin{equation}
\label{ELBO}
\mathcal{L}(\boldsymbol{\lambda}) = \int q_{\boldsymbol{\lambda}}(\boldsymbol{\theta}, x_{T+S+1} | y_{1:T+S}) \ln \left( \frac{p(y_{1:T+S},\boldsymbol{\theta}, x_{T+S+1})}{q_{\boldsymbol{\lambda}}(\boldsymbol{\theta}, x_{T+S+1} | y_{1:T+S})} \right) d\boldsymbol{\theta}dx_{1:T}.
\end{equation}
This issue should be avoidable but requires a slightly different derivation of the ELBO. Note that
\begin{align}
\ln(p(y_{T+1:T+S} | y_{1:T})) &= \int_{\theta, x_{T+S+1}} q(\theta, x_{T+1} | y_{1:T+S}) \ln(p(y_{T+1:T+S} | y_{1:T})) d_{\theta} d_{x_{T+S+1}} \nonumber \\
&= \int_{\theta, x_{T+S+1}} q(\theta, x_{T+S+1}| y_{1:T+S}) \ln \left(\frac{p(y_{T+1:T+S}, \theta, x_{T+1} | y_{1:T})}{p(x_{T+1}, \theta | y_{1:T+S})} \right) d_{\theta} d_{x_{T+S+1}}\nonumber \\
&= \int_{\theta, x_{T+S+1}} q(\theta, x_{T+S+1}| y_{1:T+S}) \nonumber \\
&\times \ln \left( \frac{p(y_{T+1:T+S}, \theta, x_{T+1} | y_{1:T})q(\theta, x_{T+S+1}| y_{1:T+S})}
{p(x_{T+1}, \theta | y_{1:T+S})q(\theta, x_{T+S+1}| y_{1:T+S}) } \right) d_{\theta} d_{x_{T+1}}\nonumber \\
&= \int_{\theta, x_{T+S+1}} q(\theta, x_{T+S+1}| y_{1:T+S}) \ln \left(\frac{p(y_{T+1:T+S}, \theta, x_{T+1} | y_{1:T})}{q(\theta, x_{T+S+1}| y_{1:T+S})} \right) d_{\theta} d_{x_{T+1}} \label{ELBOv2}\\
&+  \int_{\theta, x_{T+S+1}} q(\theta, x_{T+S+1}| y_{1:T+S}) \ln \left(\frac{q(\theta, x_{T+S+1}| y_{1:T+S})}{p(\theta, x_{T+1} | y_{1:T+S})} \right) d_{\theta} d_{x_{T+1}} \label{KL} \\
&= \mathcal{L}_2(\boldsymbol{\lambda}) + KL[q(\theta, x_{T+S+1} | y_{1:T+S}) || p(\theta, x_{T+S+1} | y_{1:T+S})]. \nonumber
\end{align}
As (\ref{KL}) is the Kullback-Leibler divergence from $q$ to $p$, it is minimised by maximising either (\ref{ELBO}) or (\ref{ELBOv2}), ie. maximising an ELBO that contains $\ln(p(y_{T+1:T+S}, \theta, x_{T+S+1} | y_{1:T}))$ instead of  $\ln(p(y_{1:T+S}, \theta, x_{T+S+1}))$ is still valid.
$\mathcal{L}_2(\boldsymbol{\lambda})$, given by (\ref{ELBOv2}), contains
\begin{equation}
\label{VBPF2}
\ln(p(y_{T+1:T+S}, \theta, x_{T+S+1} | y_{1:T})) = \ln(p(\theta | y_{1:T} )) + \ln(p(y_{T+1:T+S} | \theta, y_{1:T})) + \ln(p(x_{T+S+1} | y_{1:T+S}, \theta)).
\end{equation}
This introduces a few problems,
\begin{enumerate}
\item We do not have a parametric form of the marginal posterior distribution $p(\theta | y_{1:T})$. We do, however, have a parametric approximation available from the earlier VB: $q(\theta | y_{1:T})$. This can be feasibly used, but we would have to be comfortable with changing the model specification.
\item The particle filter estimates must be conditioned on $y_{1:T}$.
\end{enumerate}


\section{VB Particle Filtering}

We want samples from distribution $p(x_{T+1:T+S} | \theta, y_{1:T+S})$, which we decompose as
\begin{equation}
\label{posterior}
p(x_{T+1:T+S} | \theta, y_{1:T+S}) = p(x_{T+1} | y_{1:T}, \theta) p(y_{T+1} | x_{T+1}, \theta) \prod_{t=T+2}^{T+S} \left( p(y_t | x_t, \theta) p (x_t | x_{t-1}, \theta) \right)
\end{equation}
However, this is unavailable so instead we draw particles $x_{T+1:T+S}^{(1:N)}$ from a proposal distribution $f(x_{T+1:T+S})$, and then use importance sampling weights to correct the approximation. The weights for particle vector $i$ are given by
\begin{equation} 
\label{weights}
w^{(i)} \propto \frac{p(x_{T+1:T+S}^{(i)} | \theta, y_{1:T+S}, \theta)}{f(x_{T+1:T+S}^{(i)})}.
\end{equation}

A particle filter operates by positing a proposal distribution that factorises according to $f(x_{T+1:T+S}) =\prod_{t=T+1}^{T+S} f(x_{t})$, and drawing samples and calculating the weights for each time period sequentially. In this case, the weights for particles $x_{T+1}^{(i)}$ from the proposal distribution $f(x_{T+1})$ are given by
\begin{equation}
\label{weights2}
w_{T+1}^{(i)} \propto \frac{p(x_{T+1}^{(i)} | y_{1:T}, \theta) p(y_{T+1} | x_{T+1}^{(i)}, \theta)}{f(x_{T+1}^{(i)})}
\end{equation}
and weights at a future time $t$ are given by the recursion
\begin{equation}
\label{weights3}
w_t^{(i)} \propto w_{t-1}^{(i)} \frac{p(y_t | x_t^{(i)}, \theta) p (x_t^{(i)}| x_{t-1}^{(i)}, \theta)}{f(x_{t}^{(i)})}.
\end{equation}

Equation (\ref{weights2}) is problematic, we cannot easily evaluate the distribution $p(x_{T+1} | y_{1:T}, \theta)$ without running the filter over the entire dataset. As before, we can approximate this with the distribution $q(x_{T+1} | \theta, y_{1:T})$. If this is satisfactory, choose $f(x_{T+1})$ to be $q(x_{T+1} | \theta, y_{1:T})$ and each other $f(x_{t})$ to be $p(x_t | x_{t-1}, \theta)$. If the particles are resampled at each step, the weights reduce to 
\begin{equation}
\label{weights6}
w_t^i \propto p(y_t | x_t^i, \theta)
\end{equation}
for all $t$.

\section{Discussion}

Trying to update a particle filter based VB approximation introduces numerous problems, each of which seems to be solvable if we are willing to use the existing approximation of $q(\theta, x_{T+1} | y_{1:T})$ in place of the true posterior $p(\theta, x_{T+1} | y_{1:T})$. This implicitly changes the model prior used in an extremely informative way and not particularily desirable way - the prior must have the exact form that results in this particular multivariate normal posterior.
If we accept the original VB approximation as good enough then maybe this won't be such a big deal; $x_{T+S+1}$ is not included in the above substitution though there is likely to be some kind of indirect impact. 



\end{document}
