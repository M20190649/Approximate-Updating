\documentclass[12pt,a4paper]{article}%
\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)

%2multibyte Version: 5.50.0.2960 CodePage: 1252
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[centertags]{amsmath}
\usepackage{graphicx}%
\usepackage{natbib}
\usepackage{color}
\usepackage[dvipsnames,svgnames*]{xcolor}
\usepackage{array}
\usepackage[hidelinks]{hyperref}
\usepackage[font=small,skip=5pt]{caption}
\usepackage[aboveskip=2pt]{subcaption}
\usepackage{amsmath}
\usepackage{amsthm}
%\usepackage{tikz}
%\usetikzlibrary{bayesnet}
\usepackage{url}
\usepackage{ulem}
\usepackage{afterpage}
\setcounter{MaxMatrixCols}{30}
\providecommand{\U}[1]{\protect\rule{.1in}{.1in}}
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\setlength{\topmargin}{0in}
\setlength{\oddsidemargin}{0.1in}
\setlength{\evensidemargin}{0.1in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8.25in}
\numberwithin{equation}{section}
\setlength\parindent{0pt}
\begin{document}

\section{Introduction}

Consider a high frequency time series with observed values $y_{1}, \dots, y_{T+S}$, collectively denoted by $y_{1:T+S}$, with a forecaster interested in the forecast distribution of the future value $y_{T+S+h}$, where $S \geq 1$. In many applications forecasting uncertainty can be reduced by conditioning on additional data, so knowledge of the distribution of $p(y_{T+S+h} | y_{1:T+S})$ is preferable to that of $p(y_{T+S+h} | y_{1:T})$. 

Many time series models of interest contain a set of global parameters $\boldsymbol{\theta}$ and observation specific latent variables $x_{0:T}$, such that $x_t$ is assumed to follow a Markovian structure and $y_t$ is conditionally independent of $y_{1:t-1}$, given $\boldsymbol{\theta}$ and $x_t$. One of the most common computational techniques for models with latent variables is Markov Chain Monte Carlo (MCMC), which involves iterative sampling from a Markov Chain designed to converge to the true posterior distribution, however these approaches can have slow convergence rates. In this paper it is assumed that an MCMC algorithm will require a time period of length $S$ to converge, so at time $T+S$ the posterior distribution of $\boldsymbol{\theta}$  and $x_{0:T+S}$ can be conditioned only on $y_{1:T}$ and hence only $p(y_{1:T+S+h} | y_{1:T})$ is available.

The additional data $y_{T+1:T+S}$ can be included in the forecast by using filtering techniques such as the Kalman filter or particle filtering, however this will only update knowledge on $x_{0:T+S}$ but not $\boldsymbol{\theta}$. As an alternative to this method, this paper will instead approximate the posterior distribution $p(\boldsymbol{\theta}, x_{T} | y_{1:T})$ with $q_{\boldsymbol{\lambda}}(\boldsymbol{\theta}, x_{T} | y_{1:T})$ and the updated posterior distribution $p(\boldsymbol{\theta}, x_{T+S} | y_{1:T+S})$ with $q_{\boldsymbol{\lambda}}(\boldsymbol{\theta}, x_{T+S} | y_{1:T+S})$. The parameters of this approximation, denoted by $\boldsymbol{\lambda}$, are optimised by gradient descent to minimise the Kullback-Leibler divergence from $q$ to $p$, a technique known as Variational Inference, which has seen wide use in the literature (references).

Variational Bayes involves an implicit trade-off: reducing the forecast horizon of $y_{T+S+h}$ from $S+h$ to $h$ reduces the uncertainty of the forecast, but the approximation will increase the statistical error. This paper additionally explores that trade-off for a Stochastic Volatility Model, a common latent state time series model.

\begin{itemize}
\item Some motivation about forecasting and updates, can probably be copied from the MIVB file
\item Not focusing on MCMC sample
\item VB estimate at time T, VB update to time T+S
\item End goal: An approximation $q(\theta, x_{T})$ for use in forecasting.
\end{itemize}

\section{MCMC and Particle Filtering}
\begin{itemize}
\item MCMC background
\item Stochastic Volatility Model
\item PMCMC
\item Data driven particle filters?
\item Whatever else
\end{itemize}

\section{Variational Inference}


Variational Inference posits a divergence function between the true posterior distribution $p(\boldsymbol{\theta}, x_{0:T} | y_{1:T})$ and some approximating distribution $q_{\boldsymbol{\lambda}}(\boldsymbol{\theta}, x_{0:T} | y_{1:T})$, choosing the parameters $\boldsymbol{\lambda}$ for a given functional form $q$ that minimises the divergence function.

This paper will follow the traditional approach, where the divergence function is the Kullback-Leibler (KL) divergence \citep{Kullback1951} from $q_{\boldsymbol{\lambda}}(\boldsymbol{\theta}, x_{0:T} | y_{1:T})$ to the true posterior $p(\boldsymbol{\theta}, x_{0:T} | y_{1:T})$ The KL divergence is defined by
\begin{align}
\label{KL-def}
KL[q_{\boldsymbol{\lambda}}(\boldsymbol{\theta}, x_{0:T} | y_{1:T})\hspace{.1cm}||\hspace{.1cm}p(\boldsymbol{\theta}, x_{0:T} | y_{1:T})] &= \nonumber \\
\int q_{\boldsymbol{\lambda}}(\boldsymbol{\theta}, x_{0:T} | y_{1:T}) &\ln \left( \frac{q_{\boldsymbol{\lambda}}(\boldsymbol{\theta}, x_{0:T} | y_{1:T})}{p(\boldsymbol{\theta}, x_{0:T} |y_{1:T})}\right) d\boldsymbol{\theta} dx_{0:T}
\end{align}
and can alternatively be expressed as
\begin{equation}
\label{KL-ELBO}
KL[q(\boldsymbol{\theta}, x_{0:T} | \boldsymbol{\lambda})\hspace{.1cm}||\hspace{.1cm}p(\boldsymbol{\theta}, x_{0:T} | y_{1:T})] = \ln(p(y_{1:T})) - \mathcal{L}(\boldsymbol{\lambda})
\end{equation}
where $\mathcal{L}(\boldsymbol{\lambda})$ is referred to as the Evidence Lower Bound (ELBO), as it provides a lower bound on the unknown constant $\ln(p(y_{1:T}))$.  $\mathcal{L}(\boldsymbol{\lambda})$ is defined by
\begin{equation}
\label{ELBO}
\mathcal{L}(\boldsymbol{\lambda}) = \int q_{\boldsymbol{\lambda}}(\boldsymbol{\theta}, x_{0:T} | y_{1:T}) \ln \left( \frac{p(y_{1:T},\boldsymbol{\theta}, x_{0:T})}{q_{\boldsymbol{\lambda}}(\boldsymbol{\theta}, x_{0:T} | y_{1:T})} \right) d\boldsymbol{\theta}dx_{1:T},
\end{equation}
and as $\ln(p(y_{1:T}))$ is constant with respect to $\boldsymbol{\lambda}$, maximising (\ref{ELBO}) with respect to $\boldsymbol{\lambda}$ is equivalent to minimising (\ref{KL-def}). Maximising (\ref{ELBO}) is more convenient that minimising (\ref{KL-def}) as it is a function of the known joint distribution $p(y_{1:T}, \boldsymbol{\theta}, x_{0:T})$ instead of the unknown posterior $p(\boldsymbol{\theta}, x_{0:T} | y_{1:T})$.

\subsection{ELBO Optimisation}
Equation (\ref{ELBO}) can be maximised using a gradient ascent approach, where the following update step is iteratively applied until (\ref{ELBO}) converges within some pre-specified tolerance:
\begin{equation}
\label{GradAscent}
\boldsymbol{\lambda}^{(m+1)} = \boldsymbol{\lambda}^{(m)} + \rho^{(m)} \frac{\delta}{\delta\boldsymbol{\lambda}} \mathcal{L}(\boldsymbol{\lambda^{(m)}}),
\end{equation}
where the derivative is evaluated at $\boldsymbol{\lambda}^{(m)}$. This update requires some initial values $\boldsymbol{\lambda}^{(0)}$ and a sequence $\rho^{(m)}, m = 1, 2, \dots$ known as the learning rate. If $\rho^{(m)}$ is chosen to satisfy the following conditions the algorithm is guaranteed to converge to a local maximum \citep{Robbins1951}.
\begin{align}
&\sum_{m=1}^{\infty} \rho^{(m)} =  \infty \\
&\sum_{m=1}^{\infty} (\rho^{(m)})^2 <  \infty.
\end{align}

This paper uses Adam \citep{Kingma2015b} to generate the sequence $\rho^{(m)}$.

\citet{Ranganath2014} showed that a Monte Carlo estimate of the derivative of the ELBO can be given by
\begin{equation}
\label{ScoreDeriv}
\frac{\delta}{\delta\boldsymbol{\lambda}}\mathcal{L}(\boldsymbol{\lambda^{(m)}}) \approx \frac{1}{N}\sum_{i=1}^{N} \frac{\delta}{\delta\boldsymbol{\lambda}} [\ln(q_{\boldsymbol{\lambda}^{(m)}}(\boldsymbol{\theta_i}, x_{0:T, i} | y_{1:T})] \ln \left(\frac{p(y_{1:T}, \boldsymbol{\theta}_i, x_{1:T, i})}{q_{\boldsymbol{\lambda}^{(m)}}(\boldsymbol{\theta_i}, x_{0:T, i} | y_{1:T})} \right) 
\end{equation}
where $i = 1, \dots, N$ indicates independent simulations from $q_{\boldsymbol{\lambda}^{(m)}}(\boldsymbol{\theta}, x_{0:T} | y_{1:T})$. 
The terms in the sum in (\ref{ScoreDeriv}) can have large variances, and in practice a large value of $N$ is required to ensure a precise estimate of the gradient of the ELBO is obtained, slowing computation. 

The variance can be reduced by the reparameterisation trick of \citet{Kingma2014}, introducing a random vector $\boldsymbol{\epsilon}$ with a distribution $q(\boldsymbol{\epsilon})$ that contains no free parameters, and a differentiable transform $f$ such that
\begin{equation}
\label{Reparam}
f(\boldsymbol{\epsilon}, \boldsymbol{\lambda}) = \{\boldsymbol{\theta}, x_{1:T}\}.
\end{equation}
Kingma and Welling (2014) show that an $f$ exists to transform $\boldsymbol{\epsilon}$ to any continuous random variable, with examples including a location-scale transform of a standard normal $\epsilon$ and an inverse-CDF transform of a uniform $\epsilon$. In this case, (\ref{ELBO}) becomes
\begin{equation}
\label{ReparamELBO}
\mathcal{L}(\boldsymbol{\lambda}) = \int q(\boldsymbol{\epsilon}) \ln \left( \frac{p(y_{1:T},f(\boldsymbol{\epsilon}, \boldsymbol{\lambda}) |\det J(\boldsymbol{\epsilon}, \boldsymbol{\lambda})|}{q(\boldsymbol{\epsilon})} \right) d\boldsymbol{\epsilon},
\end{equation}
where $J(\boldsymbol{\epsilon}, \boldsymbol{\lambda})$ is the Jacobian matrix of the transformation $f$. The derivative of (\ref{ReparamELBO}) can be estimated by 
\begin{equation}
\label{ReparamDeriv}
\frac{\delta}{\delta \boldsymbol{\lambda}}\mathcal{L}(\boldsymbol{\lambda}^{(m)}) \approx \frac{1}{M}\sum_{i=1}^{M}
\frac{\delta f(\boldsymbol{\epsilon}_i, \boldsymbol{\lambda}^{(m)})} {\delta \boldsymbol{\lambda}}
\frac{\delta \ln (p(y_{1:T}, f(\boldsymbol{\epsilon}_i, \boldsymbol{\lambda}^{(m)})))}{\delta f(\boldsymbol{\epsilon}_i, \boldsymbol{\lambda}^{(m)})} + \frac{\delta  \ln(|\det J(\boldsymbol{\lambda}^{(m)}, \boldsymbol{\epsilon}_i)|)}{\delta \boldsymbol{\lambda}},
\end{equation} 
where simulations of $\boldsymbol{\theta}$ and $x_{1:T+S}$ are replaced by simulations of $\boldsymbol{\epsilon}$ from $q(\boldsymbol{\epsilon})$. The variance of this estimator is often orders of magnitude smaller than the estimator in (\ref{ScoreDeriv}) (citations), so $M$ can be set much lower than $N$.

\subsection{Randomised Quasi Monte Carlo}
\citep{Gunawan2017}
\citep{Matousek1998}
\citep{Bratley1988}
\citep{Sobol1967}

\subsection{Choice of $q$ distribution}

\section{Dimensionality Reduction}

ADVI simulation results

As the latent variables are Markovian, once the distribution of $x_{T+1}$ is available, the distributions of each previous $x_t, t \leq T$ is irrelevant to forecasts of $y_{T+S+h}$ and it may be more convenient to construct an approximation for $p(\boldsymbol{\theta}, x_{T+1} | y_{1:T})$ than for $p(\boldsymbol{\theta}, x_{0:T+1} | y_{1:T})$, as they contain the same information about $y_{T+S+h}$ but the dimensionality of the posterior is reduced from $T+k+2$ to $k+1$, where $k$ is the number of elements in $\boldsymbol{\theta}$. This in turn allows for more expressive approximating distributions to be used, as the size of $\boldsymbol{\lambda}$ does not grow with $T$.

In this case, using $f(\boldsymbol{\epsilon}, \boldsymbol{\lambda}) = \{\boldsymbol{\theta}, x_{T+1}\}$,
\begin{equation}
\label{DimRedLogJoint}
\ln(p(y_{1:T}, f(\boldsymbol{\epsilon}, \boldsymbol{\lambda}))) = \ln\left(\int_{X_T} p(x_{T+1} | \theta, x_T)p(x_T | y_{1:T}, \boldsymbol{\theta})d_{X_T}\right) + \ln(p(y_{1:T} |\boldsymbol{\theta})) + \ln(p(\boldsymbol{\theta}))
\end{equation}
In many cases, the distributions $p(x_T | y_{1:T}, \boldsymbol{\theta})$ and $p(y_{1:T} | \boldsymbol{\theta})$ are intractable, but \citet{TranM2017} note that the particle filter estimators $\hat{p}(x_T | y_{1:T}, \boldsymbol{\theta})$ and $\hat{p}(y_{1:T} | \boldsymbol{\theta})$ are sufficient substitutes in what they refer to as Variational Bayes with Intractable Likelihood (VBIL). The particle filter estimation of the distribution $\hat{p}(x_T | y_{1:T}, \boldsymbol{\theta})$ is expressed as a discrete set of point masses $x_T^{(k)}$ and weights $\pi_T^{(k)}$, where $k = 1, \dots, N$ represent the particles of the particle filter. In this case, the integral in (\ref{DimRedLogJoint}) reduces to the sum
\begin{equation}
\label{ParticleFilterIntegral}
\int_{x_T} p(x_{T+1} | \theta, x_T)p(x_T | y_{1:T}, \boldsymbol{\theta})d_{x_T} = \sum_{k=1}^N \pi_T^{(k)} p(x_{T+1} | \boldsymbol{\theta}, x_T^{(k)}),
\end{equation}
and hence
\begin{equation}
\label{PFLogJoint}
\ln(p(y_{1:T}, f(\boldsymbol{\epsilon}, \boldsymbol{\lambda}))) \approx \ln \left(\sum_{k=1}^N \pi_T^{(k)} p(x_{T+1} | \boldsymbol{\theta}, x_T^{(k)})\right) + \ln(\hat{p}(y_{1:T} |\boldsymbol{\theta})) + \ln(p(\boldsymbol{\theta}))
\end{equation}
can be substituted into (\ref{ReparamDeriv}). This derivative of (\ref{PFLogJoint}) with respect to $\boldsymbol{\lambda}$ can be obtained with automatic differentiation tools such as those provided by Stan \citep{Carpenter2015}.

\subsection{Importance Sampling}

\citep{Sakaya2017}


\section{Updating}

$$p(y_{1:T+S}, \theta, x_T, x_{T+S}) = p(\theta) p(y_{1:T+S} | \theta) p (x_T | \theta, y_{1:T+S}) p(x_{T+S} | x_T, \theta, y_{1:T+S})$$

\begin{itemize}
\item Proper updates with filter/smooth
\item Secondary approximation - no smoothing
\item Straight up particle filter on old approximation
\item PF on old MCMC
\item Simulation Results
\end{itemize}

\section{Empirical}

\section{Discussion}

\bibliographystyle{asa}
\bibliography{references}
 
\end{document}